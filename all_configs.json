{
    "meta-llama/Meta-Llama-3-8B": {
        "architectures": [
        "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
    },
    "meta-llama/Meta-Llama-3-8B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128009,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Prompt-Guard-86M":{
        "_name_or_path": "/tmp/tmpvlbiibjx",
        "architectures": [
          "DebertaV2ForSequenceClassification"
        ],
        "attention_probs_dropout_prob": 0.1,
        "hidden_act": "gelu",
        "hidden_dropout_prob": 0.1,
        "hidden_size": 768,
        "id2label": {
          "0": "BENIGN",
          "1": "INJECTION",
          "2": "JAILBREAK"
        },
        "initializer_range": 0.02,
        "intermediate_size": 3072,
        "label2id": {
          "BENIGN": 0,
          "INJECTION": 1,
          "JAILBREAK": 2
        },
        "layer_norm_eps": 1e-07,
        "max_position_embeddings": 512,
        "max_relative_positions": -1,
        "model_type": "deberta-v2",
        "norm_rel_ebd": "layer_norm",
        "num_attention_heads": 12,
        "num_hidden_layers": 12,
        "pad_token_id": 0,
        "pooler_dropout": 0,
        "pooler_hidden_act": "gelu",
        "pooler_hidden_size": 768,
        "pos_att_type": [
          "p2c",
          "c2p"
        ],
        "position_biased_input": false,
        "position_buckets": 256,
        "relative_attention": true,
        "share_att_key": true,
        "torch_dtype": "float32",
        "transformers_version": "4.41.2",
        "type_vocab_size": 0,
        "vocab_size": 251000
      },
      "meta-llama/Llama-Guard-3-8B-INT8":{
        "_name_or_path": "",
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "quantization_config": {
          "_load_in_4bit": false,
          "_load_in_8bit": true,
          "bnb_4bit_compute_dtype": "float32",
          "bnb_4bit_quant_storage": "uint8",
          "bnb_4bit_quant_type": "fp4",
          "bnb_4bit_use_double_quant": false,
          "llm_int8_enable_fp32_cpu_offload": false,
          "llm_int8_has_fp16_weight": false,
          "llm_int8_skip_modules": null,
          "llm_int8_threshold": 6.0,
          "load_in_4bit": false,
          "load_in_8bit": true,
          "quant_method": "bitsandbytes"
        },
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Llama-Guard-3-8B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B-Instruct-FP8":{
        "_name_or_path": "meta-llama/Meta-Llama-3.1-405B-Instruct-FP8",
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "quantization_config": {
          "activation_scale_ub": 1200.0,
          "modules_to_not_convert": [
            "model.layers.0.mlp.down_proj",
            "model.layers.0.mlp.gate_proj",
            "model.layers.0.mlp.up_proj",
            "model.layers.125.mlp.down_proj",
            "model.layers.125.mlp.gate_proj",
            "model.layers.125.mlp.up_proj",
            "model.layers.0.self_attn.k_proj",
            "model.layers.0.self_attn.o_proj",
            "model.layers.0.self_attn.q_proj",
            "model.layers.0.self_attn.v_proj",
            "model.layers.1.self_attn.k_proj",
            "model.layers.1.self_attn.o_proj",
            "model.layers.1.self_attn.q_proj",
            "model.layers.1.self_attn.v_proj",
            "model.layers.2.self_attn.k_proj",
            "model.layers.2.self_attn.o_proj",
            "model.layers.2.self_attn.q_proj",
            "model.layers.2.self_attn.v_proj",
            "model.layers.3.self_attn.k_proj",
            "model.layers.3.self_attn.o_proj",
            "model.layers.3.self_attn.q_proj",
            "model.layers.3.self_attn.v_proj",
            "model.layers.4.self_attn.k_proj",
            "model.layers.4.self_attn.o_proj",
            "model.layers.4.self_attn.q_proj",
            "model.layers.4.self_attn.v_proj",
            "model.layers.5.self_attn.k_proj",
            "model.layers.5.self_attn.o_proj",
            "model.layers.5.self_attn.q_proj",
            "model.layers.5.self_attn.v_proj",
            "model.layers.6.self_attn.k_proj",
            "model.layers.6.self_attn.o_proj",
            "model.layers.6.self_attn.q_proj",
            "model.layers.6.self_attn.v_proj",
            "model.layers.7.self_attn.k_proj",
            "model.layers.7.self_attn.o_proj",
            "model.layers.7.self_attn.q_proj",
            "model.layers.7.self_attn.v_proj",
            "model.layers.8.self_attn.k_proj",
            "model.layers.8.self_attn.o_proj",
            "model.layers.8.self_attn.q_proj",
            "model.layers.8.self_attn.v_proj",
            "model.layers.9.self_attn.k_proj",
            "model.layers.9.self_attn.o_proj",
            "model.layers.9.self_attn.q_proj",
            "model.layers.9.self_attn.v_proj",
            "model.layers.10.self_attn.k_proj",
            "model.layers.10.self_attn.o_proj",
            "model.layers.10.self_attn.q_proj",
            "model.layers.10.self_attn.v_proj",
            "model.layers.11.self_attn.k_proj",
            "model.layers.11.self_attn.o_proj",
            "model.layers.11.self_attn.q_proj",
            "model.layers.11.self_attn.v_proj",
            "model.layers.12.self_attn.k_proj",
            "model.layers.12.self_attn.o_proj",
            "model.layers.12.self_attn.q_proj",
            "model.layers.12.self_attn.v_proj",
            "model.layers.13.self_attn.k_proj",
            "model.layers.13.self_attn.o_proj",
            "model.layers.13.self_attn.q_proj",
            "model.layers.13.self_attn.v_proj",
            "model.layers.14.self_attn.k_proj",
            "model.layers.14.self_attn.o_proj",
            "model.layers.14.self_attn.q_proj",
            "model.layers.14.self_attn.v_proj",
            "model.layers.15.self_attn.k_proj",
            "model.layers.15.self_attn.o_proj",
            "model.layers.15.self_attn.q_proj",
            "model.layers.15.self_attn.v_proj",
            "model.layers.16.self_attn.k_proj",
            "model.layers.16.self_attn.o_proj",
            "model.layers.16.self_attn.q_proj",
            "model.layers.16.self_attn.v_proj",
            "model.layers.17.self_attn.k_proj",
            "model.layers.17.self_attn.o_proj",
            "model.layers.17.self_attn.q_proj",
            "model.layers.17.self_attn.v_proj",
            "model.layers.18.self_attn.k_proj",
            "model.layers.18.self_attn.o_proj",
            "model.layers.18.self_attn.q_proj",
            "model.layers.18.self_attn.v_proj",
            "model.layers.19.self_attn.k_proj",
            "model.layers.19.self_attn.o_proj",
            "model.layers.19.self_attn.q_proj",
            "model.layers.19.self_attn.v_proj",
            "model.layers.20.self_attn.k_proj",
            "model.layers.20.self_attn.o_proj",
            "model.layers.20.self_attn.q_proj",
            "model.layers.20.self_attn.v_proj",
            "model.layers.21.self_attn.k_proj",
            "model.layers.21.self_attn.o_proj",
            "model.layers.21.self_attn.q_proj",
            "model.layers.21.self_attn.v_proj",
            "model.layers.22.self_attn.k_proj",
            "model.layers.22.self_attn.o_proj",
            "model.layers.22.self_attn.q_proj",
            "model.layers.22.self_attn.v_proj",
            "model.layers.23.self_attn.k_proj",
            "model.layers.23.self_attn.o_proj",
            "model.layers.23.self_attn.q_proj",
            "model.layers.23.self_attn.v_proj",
            "model.layers.24.self_attn.k_proj",
            "model.layers.24.self_attn.o_proj",
            "model.layers.24.self_attn.q_proj",
            "model.layers.24.self_attn.v_proj",
            "model.layers.25.self_attn.k_proj",
            "model.layers.25.self_attn.o_proj",
            "model.layers.25.self_attn.q_proj",
            "model.layers.25.self_attn.v_proj",
            "model.layers.26.self_attn.k_proj",
            "model.layers.26.self_attn.o_proj",
            "model.layers.26.self_attn.q_proj",
            "model.layers.26.self_attn.v_proj",
            "model.layers.27.self_attn.k_proj",
            "model.layers.27.self_attn.o_proj",
            "model.layers.27.self_attn.q_proj",
            "model.layers.27.self_attn.v_proj",
            "model.layers.28.self_attn.k_proj",
            "model.layers.28.self_attn.o_proj",
            "model.layers.28.self_attn.q_proj",
            "model.layers.28.self_attn.v_proj",
            "model.layers.29.self_attn.k_proj",
            "model.layers.29.self_attn.o_proj",
            "model.layers.29.self_attn.q_proj",
            "model.layers.29.self_attn.v_proj",
            "model.layers.30.self_attn.k_proj",
            "model.layers.30.self_attn.o_proj",
            "model.layers.30.self_attn.q_proj",
            "model.layers.30.self_attn.v_proj",
            "model.layers.31.self_attn.k_proj",
            "model.layers.31.self_attn.o_proj",
            "model.layers.31.self_attn.q_proj",
            "model.layers.31.self_attn.v_proj",
            "model.layers.32.self_attn.k_proj",
            "model.layers.32.self_attn.o_proj",
            "model.layers.32.self_attn.q_proj",
            "model.layers.32.self_attn.v_proj",
            "model.layers.33.self_attn.k_proj",
            "model.layers.33.self_attn.o_proj",
            "model.layers.33.self_attn.q_proj",
            "model.layers.33.self_attn.v_proj",
            "model.layers.34.self_attn.k_proj",
            "model.layers.34.self_attn.o_proj",
            "model.layers.34.self_attn.q_proj",
            "model.layers.34.self_attn.v_proj",
            "model.layers.35.self_attn.k_proj",
            "model.layers.35.self_attn.o_proj",
            "model.layers.35.self_attn.q_proj",
            "model.layers.35.self_attn.v_proj",
            "model.layers.36.self_attn.k_proj",
            "model.layers.36.self_attn.o_proj",
            "model.layers.36.self_attn.q_proj",
            "model.layers.36.self_attn.v_proj",
            "model.layers.37.self_attn.k_proj",
            "model.layers.37.self_attn.o_proj",
            "model.layers.37.self_attn.q_proj",
            "model.layers.37.self_attn.v_proj",
            "model.layers.38.self_attn.k_proj",
            "model.layers.38.self_attn.o_proj",
            "model.layers.38.self_attn.q_proj",
            "model.layers.38.self_attn.v_proj",
            "model.layers.39.self_attn.k_proj",
            "model.layers.39.self_attn.o_proj",
            "model.layers.39.self_attn.q_proj",
            "model.layers.39.self_attn.v_proj",
            "model.layers.40.self_attn.k_proj",
            "model.layers.40.self_attn.o_proj",
            "model.layers.40.self_attn.q_proj",
            "model.layers.40.self_attn.v_proj",
            "model.layers.41.self_attn.k_proj",
            "model.layers.41.self_attn.o_proj",
            "model.layers.41.self_attn.q_proj",
            "model.layers.41.self_attn.v_proj",
            "model.layers.42.self_attn.k_proj",
            "model.layers.42.self_attn.o_proj",
            "model.layers.42.self_attn.q_proj",
            "model.layers.42.self_attn.v_proj",
            "model.layers.43.self_attn.k_proj",
            "model.layers.43.self_attn.o_proj",
            "model.layers.43.self_attn.q_proj",
            "model.layers.43.self_attn.v_proj",
            "model.layers.44.self_attn.k_proj",
            "model.layers.44.self_attn.o_proj",
            "model.layers.44.self_attn.q_proj",
            "model.layers.44.self_attn.v_proj",
            "model.layers.45.self_attn.k_proj",
            "model.layers.45.self_attn.o_proj",
            "model.layers.45.self_attn.q_proj",
            "model.layers.45.self_attn.v_proj",
            "model.layers.46.self_attn.k_proj",
            "model.layers.46.self_attn.o_proj",
            "model.layers.46.self_attn.q_proj",
            "model.layers.46.self_attn.v_proj",
            "model.layers.47.self_attn.k_proj",
            "model.layers.47.self_attn.o_proj",
            "model.layers.47.self_attn.q_proj",
            "model.layers.47.self_attn.v_proj",
            "model.layers.48.self_attn.k_proj",
            "model.layers.48.self_attn.o_proj",
            "model.layers.48.self_attn.q_proj",
            "model.layers.48.self_attn.v_proj",
            "model.layers.49.self_attn.k_proj",
            "model.layers.49.self_attn.o_proj",
            "model.layers.49.self_attn.q_proj",
            "model.layers.49.self_attn.v_proj",
            "model.layers.50.self_attn.k_proj",
            "model.layers.50.self_attn.o_proj",
            "model.layers.50.self_attn.q_proj",
            "model.layers.50.self_attn.v_proj",
            "model.layers.51.self_attn.k_proj",
            "model.layers.51.self_attn.o_proj",
            "model.layers.51.self_attn.q_proj",
            "model.layers.51.self_attn.v_proj",
            "model.layers.52.self_attn.k_proj",
            "model.layers.52.self_attn.o_proj",
            "model.layers.52.self_attn.q_proj",
            "model.layers.52.self_attn.v_proj",
            "model.layers.53.self_attn.k_proj",
            "model.layers.53.self_attn.o_proj",
            "model.layers.53.self_attn.q_proj",
            "model.layers.53.self_attn.v_proj",
            "model.layers.54.self_attn.k_proj",
            "model.layers.54.self_attn.o_proj",
            "model.layers.54.self_attn.q_proj",
            "model.layers.54.self_attn.v_proj",
            "model.layers.55.self_attn.k_proj",
            "model.layers.55.self_attn.o_proj",
            "model.layers.55.self_attn.q_proj",
            "model.layers.55.self_attn.v_proj",
            "model.layers.56.self_attn.k_proj",
            "model.layers.56.self_attn.o_proj",
            "model.layers.56.self_attn.q_proj",
            "model.layers.56.self_attn.v_proj",
            "model.layers.57.self_attn.k_proj",
            "model.layers.57.self_attn.o_proj",
            "model.layers.57.self_attn.q_proj",
            "model.layers.57.self_attn.v_proj",
            "model.layers.58.self_attn.k_proj",
            "model.layers.58.self_attn.o_proj",
            "model.layers.58.self_attn.q_proj",
            "model.layers.58.self_attn.v_proj",
            "model.layers.59.self_attn.k_proj",
            "model.layers.59.self_attn.o_proj",
            "model.layers.59.self_attn.q_proj",
            "model.layers.59.self_attn.v_proj",
            "model.layers.60.self_attn.k_proj",
            "model.layers.60.self_attn.o_proj",
            "model.layers.60.self_attn.q_proj",
            "model.layers.60.self_attn.v_proj",
            "model.layers.61.self_attn.k_proj",
            "model.layers.61.self_attn.o_proj",
            "model.layers.61.self_attn.q_proj",
            "model.layers.61.self_attn.v_proj",
            "model.layers.62.self_attn.k_proj",
            "model.layers.62.self_attn.o_proj",
            "model.layers.62.self_attn.q_proj",
            "model.layers.62.self_attn.v_proj",
            "model.layers.63.self_attn.k_proj",
            "model.layers.63.self_attn.o_proj",
            "model.layers.63.self_attn.q_proj",
            "model.layers.63.self_attn.v_proj",
            "model.layers.64.self_attn.k_proj",
            "model.layers.64.self_attn.o_proj",
            "model.layers.64.self_attn.q_proj",
            "model.layers.64.self_attn.v_proj",
            "model.layers.65.self_attn.k_proj",
            "model.layers.65.self_attn.o_proj",
            "model.layers.65.self_attn.q_proj",
            "model.layers.65.self_attn.v_proj",
            "model.layers.66.self_attn.k_proj",
            "model.layers.66.self_attn.o_proj",
            "model.layers.66.self_attn.q_proj",
            "model.layers.66.self_attn.v_proj",
            "model.layers.67.self_attn.k_proj",
            "model.layers.67.self_attn.o_proj",
            "model.layers.67.self_attn.q_proj",
            "model.layers.67.self_attn.v_proj",
            "model.layers.68.self_attn.k_proj",
            "model.layers.68.self_attn.o_proj",
            "model.layers.68.self_attn.q_proj",
            "model.layers.68.self_attn.v_proj",
            "model.layers.69.self_attn.k_proj",
            "model.layers.69.self_attn.o_proj",
            "model.layers.69.self_attn.q_proj",
            "model.layers.69.self_attn.v_proj",
            "model.layers.70.self_attn.k_proj",
            "model.layers.70.self_attn.o_proj",
            "model.layers.70.self_attn.q_proj",
            "model.layers.70.self_attn.v_proj",
            "model.layers.71.self_attn.k_proj",
            "model.layers.71.self_attn.o_proj",
            "model.layers.71.self_attn.q_proj",
            "model.layers.71.self_attn.v_proj",
            "model.layers.72.self_attn.k_proj",
            "model.layers.72.self_attn.o_proj",
            "model.layers.72.self_attn.q_proj",
            "model.layers.72.self_attn.v_proj",
            "model.layers.73.self_attn.k_proj",
            "model.layers.73.self_attn.o_proj",
            "model.layers.73.self_attn.q_proj",
            "model.layers.73.self_attn.v_proj",
            "model.layers.74.self_attn.k_proj",
            "model.layers.74.self_attn.o_proj",
            "model.layers.74.self_attn.q_proj",
            "model.layers.74.self_attn.v_proj",
            "model.layers.75.self_attn.k_proj",
            "model.layers.75.self_attn.o_proj",
            "model.layers.75.self_attn.q_proj",
            "model.layers.75.self_attn.v_proj",
            "model.layers.76.self_attn.k_proj",
            "model.layers.76.self_attn.o_proj",
            "model.layers.76.self_attn.q_proj",
            "model.layers.76.self_attn.v_proj",
            "model.layers.77.self_attn.k_proj",
            "model.layers.77.self_attn.o_proj",
            "model.layers.77.self_attn.q_proj",
            "model.layers.77.self_attn.v_proj",
            "model.layers.78.self_attn.k_proj",
            "model.layers.78.self_attn.o_proj",
            "model.layers.78.self_attn.q_proj",
            "model.layers.78.self_attn.v_proj",
            "model.layers.79.self_attn.k_proj",
            "model.layers.79.self_attn.o_proj",
            "model.layers.79.self_attn.q_proj",
            "model.layers.79.self_attn.v_proj",
            "model.layers.80.self_attn.k_proj",
            "model.layers.80.self_attn.o_proj",
            "model.layers.80.self_attn.q_proj",
            "model.layers.80.self_attn.v_proj",
            "model.layers.81.self_attn.k_proj",
            "model.layers.81.self_attn.o_proj",
            "model.layers.81.self_attn.q_proj",
            "model.layers.81.self_attn.v_proj",
            "model.layers.82.self_attn.k_proj",
            "model.layers.82.self_attn.o_proj",
            "model.layers.82.self_attn.q_proj",
            "model.layers.82.self_attn.v_proj",
            "model.layers.83.self_attn.k_proj",
            "model.layers.83.self_attn.o_proj",
            "model.layers.83.self_attn.q_proj",
            "model.layers.83.self_attn.v_proj",
            "model.layers.84.self_attn.k_proj",
            "model.layers.84.self_attn.o_proj",
            "model.layers.84.self_attn.q_proj",
            "model.layers.84.self_attn.v_proj",
            "model.layers.85.self_attn.k_proj",
            "model.layers.85.self_attn.o_proj",
            "model.layers.85.self_attn.q_proj",
            "model.layers.85.self_attn.v_proj",
            "model.layers.86.self_attn.k_proj",
            "model.layers.86.self_attn.o_proj",
            "model.layers.86.self_attn.q_proj",
            "model.layers.86.self_attn.v_proj",
            "model.layers.87.self_attn.k_proj",
            "model.layers.87.self_attn.o_proj",
            "model.layers.87.self_attn.q_proj",
            "model.layers.87.self_attn.v_proj",
            "model.layers.88.self_attn.k_proj",
            "model.layers.88.self_attn.o_proj",
            "model.layers.88.self_attn.q_proj",
            "model.layers.88.self_attn.v_proj",
            "model.layers.89.self_attn.k_proj",
            "model.layers.89.self_attn.o_proj",
            "model.layers.89.self_attn.q_proj",
            "model.layers.89.self_attn.v_proj",
            "model.layers.90.self_attn.k_proj",
            "model.layers.90.self_attn.o_proj",
            "model.layers.90.self_attn.q_proj",
            "model.layers.90.self_attn.v_proj",
            "model.layers.91.self_attn.k_proj",
            "model.layers.91.self_attn.o_proj",
            "model.layers.91.self_attn.q_proj",
            "model.layers.91.self_attn.v_proj",
            "model.layers.92.self_attn.k_proj",
            "model.layers.92.self_attn.o_proj",
            "model.layers.92.self_attn.q_proj",
            "model.layers.92.self_attn.v_proj",
            "model.layers.93.self_attn.k_proj",
            "model.layers.93.self_attn.o_proj",
            "model.layers.93.self_attn.q_proj",
            "model.layers.93.self_attn.v_proj",
            "model.layers.94.self_attn.k_proj",
            "model.layers.94.self_attn.o_proj",
            "model.layers.94.self_attn.q_proj",
            "model.layers.94.self_attn.v_proj",
            "model.layers.95.self_attn.k_proj",
            "model.layers.95.self_attn.o_proj",
            "model.layers.95.self_attn.q_proj",
            "model.layers.95.self_attn.v_proj",
            "model.layers.96.self_attn.k_proj",
            "model.layers.96.self_attn.o_proj",
            "model.layers.96.self_attn.q_proj",
            "model.layers.96.self_attn.v_proj",
            "model.layers.97.self_attn.k_proj",
            "model.layers.97.self_attn.o_proj",
            "model.layers.97.self_attn.q_proj",
            "model.layers.97.self_attn.v_proj",
            "model.layers.98.self_attn.k_proj",
            "model.layers.98.self_attn.o_proj",
            "model.layers.98.self_attn.q_proj",
            "model.layers.98.self_attn.v_proj",
            "model.layers.99.self_attn.k_proj",
            "model.layers.99.self_attn.o_proj",
            "model.layers.99.self_attn.q_proj",
            "model.layers.99.self_attn.v_proj",
            "model.layers.100.self_attn.k_proj",
            "model.layers.100.self_attn.o_proj",
            "model.layers.100.self_attn.q_proj",
            "model.layers.100.self_attn.v_proj",
            "model.layers.101.self_attn.k_proj",
            "model.layers.101.self_attn.o_proj",
            "model.layers.101.self_attn.q_proj",
            "model.layers.101.self_attn.v_proj",
            "model.layers.102.self_attn.k_proj",
            "model.layers.102.self_attn.o_proj",
            "model.layers.102.self_attn.q_proj",
            "model.layers.102.self_attn.v_proj",
            "model.layers.103.self_attn.k_proj",
            "model.layers.103.self_attn.o_proj",
            "model.layers.103.self_attn.q_proj",
            "model.layers.103.self_attn.v_proj",
            "model.layers.104.self_attn.k_proj",
            "model.layers.104.self_attn.o_proj",
            "model.layers.104.self_attn.q_proj",
            "model.layers.104.self_attn.v_proj",
            "model.layers.105.self_attn.k_proj",
            "model.layers.105.self_attn.o_proj",
            "model.layers.105.self_attn.q_proj",
            "model.layers.105.self_attn.v_proj",
            "model.layers.106.self_attn.k_proj",
            "model.layers.106.self_attn.o_proj",
            "model.layers.106.self_attn.q_proj",
            "model.layers.106.self_attn.v_proj",
            "model.layers.107.self_attn.k_proj",
            "model.layers.107.self_attn.o_proj",
            "model.layers.107.self_attn.q_proj",
            "model.layers.107.self_attn.v_proj",
            "model.layers.108.self_attn.k_proj",
            "model.layers.108.self_attn.o_proj",
            "model.layers.108.self_attn.q_proj",
            "model.layers.108.self_attn.v_proj",
            "model.layers.109.self_attn.k_proj",
            "model.layers.109.self_attn.o_proj",
            "model.layers.109.self_attn.q_proj",
            "model.layers.109.self_attn.v_proj",
            "model.layers.110.self_attn.k_proj",
            "model.layers.110.self_attn.o_proj",
            "model.layers.110.self_attn.q_proj",
            "model.layers.110.self_attn.v_proj",
            "model.layers.111.self_attn.k_proj",
            "model.layers.111.self_attn.o_proj",
            "model.layers.111.self_attn.q_proj",
            "model.layers.111.self_attn.v_proj",
            "model.layers.112.self_attn.k_proj",
            "model.layers.112.self_attn.o_proj",
            "model.layers.112.self_attn.q_proj",
            "model.layers.112.self_attn.v_proj",
            "model.layers.113.self_attn.k_proj",
            "model.layers.113.self_attn.o_proj",
            "model.layers.113.self_attn.q_proj",
            "model.layers.113.self_attn.v_proj",
            "model.layers.114.self_attn.k_proj",
            "model.layers.114.self_attn.o_proj",
            "model.layers.114.self_attn.q_proj",
            "model.layers.114.self_attn.v_proj",
            "model.layers.115.self_attn.k_proj",
            "model.layers.115.self_attn.o_proj",
            "model.layers.115.self_attn.q_proj",
            "model.layers.115.self_attn.v_proj",
            "model.layers.116.self_attn.k_proj",
            "model.layers.116.self_attn.o_proj",
            "model.layers.116.self_attn.q_proj",
            "model.layers.116.self_attn.v_proj",
            "model.layers.117.self_attn.k_proj",
            "model.layers.117.self_attn.o_proj",
            "model.layers.117.self_attn.q_proj",
            "model.layers.117.self_attn.v_proj",
            "model.layers.118.self_attn.k_proj",
            "model.layers.118.self_attn.o_proj",
            "model.layers.118.self_attn.q_proj",
            "model.layers.118.self_attn.v_proj",
            "model.layers.119.self_attn.k_proj",
            "model.layers.119.self_attn.o_proj",
            "model.layers.119.self_attn.q_proj",
            "model.layers.119.self_attn.v_proj",
            "model.layers.120.self_attn.k_proj",
            "model.layers.120.self_attn.o_proj",
            "model.layers.120.self_attn.q_proj",
            "model.layers.120.self_attn.v_proj",
            "model.layers.121.self_attn.k_proj",
            "model.layers.121.self_attn.o_proj",
            "model.layers.121.self_attn.q_proj",
            "model.layers.121.self_attn.v_proj",
            "model.layers.122.self_attn.k_proj",
            "model.layers.122.self_attn.o_proj",
            "model.layers.122.self_attn.q_proj",
            "model.layers.122.self_attn.v_proj",
            "model.layers.123.self_attn.k_proj",
            "model.layers.123.self_attn.o_proj",
            "model.layers.123.self_attn.q_proj",
            "model.layers.123.self_attn.v_proj",
            "model.layers.124.self_attn.k_proj",
            "model.layers.124.self_attn.o_proj",
            "model.layers.124.self_attn.q_proj",
            "model.layers.124.self_attn.v_proj",
            "model.layers.125.self_attn.k_proj",
            "model.layers.125.self_attn.o_proj",
            "model.layers.125.self_attn.q_proj",
            "model.layers.125.self_attn.v_proj"
          ],
          "quant_method": "fbgemm_fp8"
        },
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B-FP8":{
        "_name_or_path": "meta-llama/Meta-Llama-3.1-405B-FP8",
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "quantization_config": {
          "activation_scale_ub": 1200.0,
          "modules_to_not_convert": [
            "lm_head",
            "model.layers.0.mlp.down_proj",
            "model.layers.0.mlp.gate_proj",
            "model.layers.0.mlp.up_proj",
            "model.layers.125.mlp.down_proj",
            "model.layers.125.mlp.gate_proj",
            "model.layers.125.mlp.up_proj",
            "model.layers.0.self_attn.k_proj",
            "model.layers.0.self_attn.o_proj",
            "model.layers.0.self_attn.q_proj",
            "model.layers.0.self_attn.v_proj",
            "model.layers.1.self_attn.k_proj",
            "model.layers.1.self_attn.o_proj",
            "model.layers.1.self_attn.q_proj",
            "model.layers.1.self_attn.v_proj",
            "model.layers.2.self_attn.k_proj",
            "model.layers.2.self_attn.o_proj",
            "model.layers.2.self_attn.q_proj",
            "model.layers.2.self_attn.v_proj",
            "model.layers.3.self_attn.k_proj",
            "model.layers.3.self_attn.o_proj",
            "model.layers.3.self_attn.q_proj",
            "model.layers.3.self_attn.v_proj",
            "model.layers.4.self_attn.k_proj",
            "model.layers.4.self_attn.o_proj",
            "model.layers.4.self_attn.q_proj",
            "model.layers.4.self_attn.v_proj",
            "model.layers.5.self_attn.k_proj",
            "model.layers.5.self_attn.o_proj",
            "model.layers.5.self_attn.q_proj",
            "model.layers.5.self_attn.v_proj",
            "model.layers.6.self_attn.k_proj",
            "model.layers.6.self_attn.o_proj",
            "model.layers.6.self_attn.q_proj",
            "model.layers.6.self_attn.v_proj",
            "model.layers.7.self_attn.k_proj",
            "model.layers.7.self_attn.o_proj",
            "model.layers.7.self_attn.q_proj",
            "model.layers.7.self_attn.v_proj",
            "model.layers.8.self_attn.k_proj",
            "model.layers.8.self_attn.o_proj",
            "model.layers.8.self_attn.q_proj",
            "model.layers.8.self_attn.v_proj",
            "model.layers.9.self_attn.k_proj",
            "model.layers.9.self_attn.o_proj",
            "model.layers.9.self_attn.q_proj",
            "model.layers.9.self_attn.v_proj",
            "model.layers.10.self_attn.k_proj",
            "model.layers.10.self_attn.o_proj",
            "model.layers.10.self_attn.q_proj",
            "model.layers.10.self_attn.v_proj",
            "model.layers.11.self_attn.k_proj",
            "model.layers.11.self_attn.o_proj",
            "model.layers.11.self_attn.q_proj",
            "model.layers.11.self_attn.v_proj",
            "model.layers.12.self_attn.k_proj",
            "model.layers.12.self_attn.o_proj",
            "model.layers.12.self_attn.q_proj",
            "model.layers.12.self_attn.v_proj",
            "model.layers.13.self_attn.k_proj",
            "model.layers.13.self_attn.o_proj",
            "model.layers.13.self_attn.q_proj",
            "model.layers.13.self_attn.v_proj",
            "model.layers.14.self_attn.k_proj",
            "model.layers.14.self_attn.o_proj",
            "model.layers.14.self_attn.q_proj",
            "model.layers.14.self_attn.v_proj",
            "model.layers.15.self_attn.k_proj",
            "model.layers.15.self_attn.o_proj",
            "model.layers.15.self_attn.q_proj",
            "model.layers.15.self_attn.v_proj",
            "model.layers.16.self_attn.k_proj",
            "model.layers.16.self_attn.o_proj",
            "model.layers.16.self_attn.q_proj",
            "model.layers.16.self_attn.v_proj",
            "model.layers.17.self_attn.k_proj",
            "model.layers.17.self_attn.o_proj",
            "model.layers.17.self_attn.q_proj",
            "model.layers.17.self_attn.v_proj",
            "model.layers.18.self_attn.k_proj",
            "model.layers.18.self_attn.o_proj",
            "model.layers.18.self_attn.q_proj",
            "model.layers.18.self_attn.v_proj",
            "model.layers.19.self_attn.k_proj",
            "model.layers.19.self_attn.o_proj",
            "model.layers.19.self_attn.q_proj",
            "model.layers.19.self_attn.v_proj",
            "model.layers.20.self_attn.k_proj",
            "model.layers.20.self_attn.o_proj",
            "model.layers.20.self_attn.q_proj",
            "model.layers.20.self_attn.v_proj",
            "model.layers.21.self_attn.k_proj",
            "model.layers.21.self_attn.o_proj",
            "model.layers.21.self_attn.q_proj",
            "model.layers.21.self_attn.v_proj",
            "model.layers.22.self_attn.k_proj",
            "model.layers.22.self_attn.o_proj",
            "model.layers.22.self_attn.q_proj",
            "model.layers.22.self_attn.v_proj",
            "model.layers.23.self_attn.k_proj",
            "model.layers.23.self_attn.o_proj",
            "model.layers.23.self_attn.q_proj",
            "model.layers.23.self_attn.v_proj",
            "model.layers.24.self_attn.k_proj",
            "model.layers.24.self_attn.o_proj",
            "model.layers.24.self_attn.q_proj",
            "model.layers.24.self_attn.v_proj",
            "model.layers.25.self_attn.k_proj",
            "model.layers.25.self_attn.o_proj",
            "model.layers.25.self_attn.q_proj",
            "model.layers.25.self_attn.v_proj",
            "model.layers.26.self_attn.k_proj",
            "model.layers.26.self_attn.o_proj",
            "model.layers.26.self_attn.q_proj",
            "model.layers.26.self_attn.v_proj",
            "model.layers.27.self_attn.k_proj",
            "model.layers.27.self_attn.o_proj",
            "model.layers.27.self_attn.q_proj",
            "model.layers.27.self_attn.v_proj",
            "model.layers.28.self_attn.k_proj",
            "model.layers.28.self_attn.o_proj",
            "model.layers.28.self_attn.q_proj",
            "model.layers.28.self_attn.v_proj",
            "model.layers.29.self_attn.k_proj",
            "model.layers.29.self_attn.o_proj",
            "model.layers.29.self_attn.q_proj",
            "model.layers.29.self_attn.v_proj",
            "model.layers.30.self_attn.k_proj",
            "model.layers.30.self_attn.o_proj",
            "model.layers.30.self_attn.q_proj",
            "model.layers.30.self_attn.v_proj",
            "model.layers.31.self_attn.k_proj",
            "model.layers.31.self_attn.o_proj",
            "model.layers.31.self_attn.q_proj",
            "model.layers.31.self_attn.v_proj",
            "model.layers.32.self_attn.k_proj",
            "model.layers.32.self_attn.o_proj",
            "model.layers.32.self_attn.q_proj",
            "model.layers.32.self_attn.v_proj",
            "model.layers.33.self_attn.k_proj",
            "model.layers.33.self_attn.o_proj",
            "model.layers.33.self_attn.q_proj",
            "model.layers.33.self_attn.v_proj",
            "model.layers.34.self_attn.k_proj",
            "model.layers.34.self_attn.o_proj",
            "model.layers.34.self_attn.q_proj",
            "model.layers.34.self_attn.v_proj",
            "model.layers.35.self_attn.k_proj",
            "model.layers.35.self_attn.o_proj",
            "model.layers.35.self_attn.q_proj",
            "model.layers.35.self_attn.v_proj",
            "model.layers.36.self_attn.k_proj",
            "model.layers.36.self_attn.o_proj",
            "model.layers.36.self_attn.q_proj",
            "model.layers.36.self_attn.v_proj",
            "model.layers.37.self_attn.k_proj",
            "model.layers.37.self_attn.o_proj",
            "model.layers.37.self_attn.q_proj",
            "model.layers.37.self_attn.v_proj",
            "model.layers.38.self_attn.k_proj",
            "model.layers.38.self_attn.o_proj",
            "model.layers.38.self_attn.q_proj",
            "model.layers.38.self_attn.v_proj",
            "model.layers.39.self_attn.k_proj",
            "model.layers.39.self_attn.o_proj",
            "model.layers.39.self_attn.q_proj",
            "model.layers.39.self_attn.v_proj",
            "model.layers.40.self_attn.k_proj",
            "model.layers.40.self_attn.o_proj",
            "model.layers.40.self_attn.q_proj",
            "model.layers.40.self_attn.v_proj",
            "model.layers.41.self_attn.k_proj",
            "model.layers.41.self_attn.o_proj",
            "model.layers.41.self_attn.q_proj",
            "model.layers.41.self_attn.v_proj",
            "model.layers.42.self_attn.k_proj",
            "model.layers.42.self_attn.o_proj",
            "model.layers.42.self_attn.q_proj",
            "model.layers.42.self_attn.v_proj",
            "model.layers.43.self_attn.k_proj",
            "model.layers.43.self_attn.o_proj",
            "model.layers.43.self_attn.q_proj",
            "model.layers.43.self_attn.v_proj",
            "model.layers.44.self_attn.k_proj",
            "model.layers.44.self_attn.o_proj",
            "model.layers.44.self_attn.q_proj",
            "model.layers.44.self_attn.v_proj",
            "model.layers.45.self_attn.k_proj",
            "model.layers.45.self_attn.o_proj",
            "model.layers.45.self_attn.q_proj",
            "model.layers.45.self_attn.v_proj",
            "model.layers.46.self_attn.k_proj",
            "model.layers.46.self_attn.o_proj",
            "model.layers.46.self_attn.q_proj",
            "model.layers.46.self_attn.v_proj",
            "model.layers.47.self_attn.k_proj",
            "model.layers.47.self_attn.o_proj",
            "model.layers.47.self_attn.q_proj",
            "model.layers.47.self_attn.v_proj",
            "model.layers.48.self_attn.k_proj",
            "model.layers.48.self_attn.o_proj",
            "model.layers.48.self_attn.q_proj",
            "model.layers.48.self_attn.v_proj",
            "model.layers.49.self_attn.k_proj",
            "model.layers.49.self_attn.o_proj",
            "model.layers.49.self_attn.q_proj",
            "model.layers.49.self_attn.v_proj",
            "model.layers.50.self_attn.k_proj",
            "model.layers.50.self_attn.o_proj",
            "model.layers.50.self_attn.q_proj",
            "model.layers.50.self_attn.v_proj",
            "model.layers.51.self_attn.k_proj",
            "model.layers.51.self_attn.o_proj",
            "model.layers.51.self_attn.q_proj",
            "model.layers.51.self_attn.v_proj",
            "model.layers.52.self_attn.k_proj",
            "model.layers.52.self_attn.o_proj",
            "model.layers.52.self_attn.q_proj",
            "model.layers.52.self_attn.v_proj",
            "model.layers.53.self_attn.k_proj",
            "model.layers.53.self_attn.o_proj",
            "model.layers.53.self_attn.q_proj",
            "model.layers.53.self_attn.v_proj",
            "model.layers.54.self_attn.k_proj",
            "model.layers.54.self_attn.o_proj",
            "model.layers.54.self_attn.q_proj",
            "model.layers.54.self_attn.v_proj",
            "model.layers.55.self_attn.k_proj",
            "model.layers.55.self_attn.o_proj",
            "model.layers.55.self_attn.q_proj",
            "model.layers.55.self_attn.v_proj",
            "model.layers.56.self_attn.k_proj",
            "model.layers.56.self_attn.o_proj",
            "model.layers.56.self_attn.q_proj",
            "model.layers.56.self_attn.v_proj",
            "model.layers.57.self_attn.k_proj",
            "model.layers.57.self_attn.o_proj",
            "model.layers.57.self_attn.q_proj",
            "model.layers.57.self_attn.v_proj",
            "model.layers.58.self_attn.k_proj",
            "model.layers.58.self_attn.o_proj",
            "model.layers.58.self_attn.q_proj",
            "model.layers.58.self_attn.v_proj",
            "model.layers.59.self_attn.k_proj",
            "model.layers.59.self_attn.o_proj",
            "model.layers.59.self_attn.q_proj",
            "model.layers.59.self_attn.v_proj",
            "model.layers.60.self_attn.k_proj",
            "model.layers.60.self_attn.o_proj",
            "model.layers.60.self_attn.q_proj",
            "model.layers.60.self_attn.v_proj",
            "model.layers.61.self_attn.k_proj",
            "model.layers.61.self_attn.o_proj",
            "model.layers.61.self_attn.q_proj",
            "model.layers.61.self_attn.v_proj",
            "model.layers.62.self_attn.k_proj",
            "model.layers.62.self_attn.o_proj",
            "model.layers.62.self_attn.q_proj",
            "model.layers.62.self_attn.v_proj",
            "model.layers.63.self_attn.k_proj",
            "model.layers.63.self_attn.o_proj",
            "model.layers.63.self_attn.q_proj",
            "model.layers.63.self_attn.v_proj",
            "model.layers.64.self_attn.k_proj",
            "model.layers.64.self_attn.o_proj",
            "model.layers.64.self_attn.q_proj",
            "model.layers.64.self_attn.v_proj",
            "model.layers.65.self_attn.k_proj",
            "model.layers.65.self_attn.o_proj",
            "model.layers.65.self_attn.q_proj",
            "model.layers.65.self_attn.v_proj",
            "model.layers.66.self_attn.k_proj",
            "model.layers.66.self_attn.o_proj",
            "model.layers.66.self_attn.q_proj",
            "model.layers.66.self_attn.v_proj",
            "model.layers.67.self_attn.k_proj",
            "model.layers.67.self_attn.o_proj",
            "model.layers.67.self_attn.q_proj",
            "model.layers.67.self_attn.v_proj",
            "model.layers.68.self_attn.k_proj",
            "model.layers.68.self_attn.o_proj",
            "model.layers.68.self_attn.q_proj",
            "model.layers.68.self_attn.v_proj",
            "model.layers.69.self_attn.k_proj",
            "model.layers.69.self_attn.o_proj",
            "model.layers.69.self_attn.q_proj",
            "model.layers.69.self_attn.v_proj",
            "model.layers.70.self_attn.k_proj",
            "model.layers.70.self_attn.o_proj",
            "model.layers.70.self_attn.q_proj",
            "model.layers.70.self_attn.v_proj",
            "model.layers.71.self_attn.k_proj",
            "model.layers.71.self_attn.o_proj",
            "model.layers.71.self_attn.q_proj",
            "model.layers.71.self_attn.v_proj",
            "model.layers.72.self_attn.k_proj",
            "model.layers.72.self_attn.o_proj",
            "model.layers.72.self_attn.q_proj",
            "model.layers.72.self_attn.v_proj",
            "model.layers.73.self_attn.k_proj",
            "model.layers.73.self_attn.o_proj",
            "model.layers.73.self_attn.q_proj",
            "model.layers.73.self_attn.v_proj",
            "model.layers.74.self_attn.k_proj",
            "model.layers.74.self_attn.o_proj",
            "model.layers.74.self_attn.q_proj",
            "model.layers.74.self_attn.v_proj",
            "model.layers.75.self_attn.k_proj",
            "model.layers.75.self_attn.o_proj",
            "model.layers.75.self_attn.q_proj",
            "model.layers.75.self_attn.v_proj",
            "model.layers.76.self_attn.k_proj",
            "model.layers.76.self_attn.o_proj",
            "model.layers.76.self_attn.q_proj",
            "model.layers.76.self_attn.v_proj",
            "model.layers.77.self_attn.k_proj",
            "model.layers.77.self_attn.o_proj",
            "model.layers.77.self_attn.q_proj",
            "model.layers.77.self_attn.v_proj",
            "model.layers.78.self_attn.k_proj",
            "model.layers.78.self_attn.o_proj",
            "model.layers.78.self_attn.q_proj",
            "model.layers.78.self_attn.v_proj",
            "model.layers.79.self_attn.k_proj",
            "model.layers.79.self_attn.o_proj",
            "model.layers.79.self_attn.q_proj",
            "model.layers.79.self_attn.v_proj",
            "model.layers.80.self_attn.k_proj",
            "model.layers.80.self_attn.o_proj",
            "model.layers.80.self_attn.q_proj",
            "model.layers.80.self_attn.v_proj",
            "model.layers.81.self_attn.k_proj",
            "model.layers.81.self_attn.o_proj",
            "model.layers.81.self_attn.q_proj",
            "model.layers.81.self_attn.v_proj",
            "model.layers.82.self_attn.k_proj",
            "model.layers.82.self_attn.o_proj",
            "model.layers.82.self_attn.q_proj",
            "model.layers.82.self_attn.v_proj",
            "model.layers.83.self_attn.k_proj",
            "model.layers.83.self_attn.o_proj",
            "model.layers.83.self_attn.q_proj",
            "model.layers.83.self_attn.v_proj",
            "model.layers.84.self_attn.k_proj",
            "model.layers.84.self_attn.o_proj",
            "model.layers.84.self_attn.q_proj",
            "model.layers.84.self_attn.v_proj",
            "model.layers.85.self_attn.k_proj",
            "model.layers.85.self_attn.o_proj",
            "model.layers.85.self_attn.q_proj",
            "model.layers.85.self_attn.v_proj",
            "model.layers.86.self_attn.k_proj",
            "model.layers.86.self_attn.o_proj",
            "model.layers.86.self_attn.q_proj",
            "model.layers.86.self_attn.v_proj",
            "model.layers.87.self_attn.k_proj",
            "model.layers.87.self_attn.o_proj",
            "model.layers.87.self_attn.q_proj",
            "model.layers.87.self_attn.v_proj",
            "model.layers.88.self_attn.k_proj",
            "model.layers.88.self_attn.o_proj",
            "model.layers.88.self_attn.q_proj",
            "model.layers.88.self_attn.v_proj",
            "model.layers.89.self_attn.k_proj",
            "model.layers.89.self_attn.o_proj",
            "model.layers.89.self_attn.q_proj",
            "model.layers.89.self_attn.v_proj",
            "model.layers.90.self_attn.k_proj",
            "model.layers.90.self_attn.o_proj",
            "model.layers.90.self_attn.q_proj",
            "model.layers.90.self_attn.v_proj",
            "model.layers.91.self_attn.k_proj",
            "model.layers.91.self_attn.o_proj",
            "model.layers.91.self_attn.q_proj",
            "model.layers.91.self_attn.v_proj",
            "model.layers.92.self_attn.k_proj",
            "model.layers.92.self_attn.o_proj",
            "model.layers.92.self_attn.q_proj",
            "model.layers.92.self_attn.v_proj",
            "model.layers.93.self_attn.k_proj",
            "model.layers.93.self_attn.o_proj",
            "model.layers.93.self_attn.q_proj",
            "model.layers.93.self_attn.v_proj",
            "model.layers.94.self_attn.k_proj",
            "model.layers.94.self_attn.o_proj",
            "model.layers.94.self_attn.q_proj",
            "model.layers.94.self_attn.v_proj",
            "model.layers.95.self_attn.k_proj",
            "model.layers.95.self_attn.o_proj",
            "model.layers.95.self_attn.q_proj",
            "model.layers.95.self_attn.v_proj",
            "model.layers.96.self_attn.k_proj",
            "model.layers.96.self_attn.o_proj",
            "model.layers.96.self_attn.q_proj",
            "model.layers.96.self_attn.v_proj",
            "model.layers.97.self_attn.k_proj",
            "model.layers.97.self_attn.o_proj",
            "model.layers.97.self_attn.q_proj",
            "model.layers.97.self_attn.v_proj",
            "model.layers.98.self_attn.k_proj",
            "model.layers.98.self_attn.o_proj",
            "model.layers.98.self_attn.q_proj",
            "model.layers.98.self_attn.v_proj",
            "model.layers.99.self_attn.k_proj",
            "model.layers.99.self_attn.o_proj",
            "model.layers.99.self_attn.q_proj",
            "model.layers.99.self_attn.v_proj",
            "model.layers.100.self_attn.k_proj",
            "model.layers.100.self_attn.o_proj",
            "model.layers.100.self_attn.q_proj",
            "model.layers.100.self_attn.v_proj",
            "model.layers.101.self_attn.k_proj",
            "model.layers.101.self_attn.o_proj",
            "model.layers.101.self_attn.q_proj",
            "model.layers.101.self_attn.v_proj",
            "model.layers.102.self_attn.k_proj",
            "model.layers.102.self_attn.o_proj",
            "model.layers.102.self_attn.q_proj",
            "model.layers.102.self_attn.v_proj",
            "model.layers.103.self_attn.k_proj",
            "model.layers.103.self_attn.o_proj",
            "model.layers.103.self_attn.q_proj",
            "model.layers.103.self_attn.v_proj",
            "model.layers.104.self_attn.k_proj",
            "model.layers.104.self_attn.o_proj",
            "model.layers.104.self_attn.q_proj",
            "model.layers.104.self_attn.v_proj",
            "model.layers.105.self_attn.k_proj",
            "model.layers.105.self_attn.o_proj",
            "model.layers.105.self_attn.q_proj",
            "model.layers.105.self_attn.v_proj",
            "model.layers.106.self_attn.k_proj",
            "model.layers.106.self_attn.o_proj",
            "model.layers.106.self_attn.q_proj",
            "model.layers.106.self_attn.v_proj",
            "model.layers.107.self_attn.k_proj",
            "model.layers.107.self_attn.o_proj",
            "model.layers.107.self_attn.q_proj",
            "model.layers.107.self_attn.v_proj",
            "model.layers.108.self_attn.k_proj",
            "model.layers.108.self_attn.o_proj",
            "model.layers.108.self_attn.q_proj",
            "model.layers.108.self_attn.v_proj",
            "model.layers.109.self_attn.k_proj",
            "model.layers.109.self_attn.o_proj",
            "model.layers.109.self_attn.q_proj",
            "model.layers.109.self_attn.v_proj",
            "model.layers.110.self_attn.k_proj",
            "model.layers.110.self_attn.o_proj",
            "model.layers.110.self_attn.q_proj",
            "model.layers.110.self_attn.v_proj",
            "model.layers.111.self_attn.k_proj",
            "model.layers.111.self_attn.o_proj",
            "model.layers.111.self_attn.q_proj",
            "model.layers.111.self_attn.v_proj",
            "model.layers.112.self_attn.k_proj",
            "model.layers.112.self_attn.o_proj",
            "model.layers.112.self_attn.q_proj",
            "model.layers.112.self_attn.v_proj",
            "model.layers.113.self_attn.k_proj",
            "model.layers.113.self_attn.o_proj",
            "model.layers.113.self_attn.q_proj",
            "model.layers.113.self_attn.v_proj",
            "model.layers.114.self_attn.k_proj",
            "model.layers.114.self_attn.o_proj",
            "model.layers.114.self_attn.q_proj",
            "model.layers.114.self_attn.v_proj",
            "model.layers.115.self_attn.k_proj",
            "model.layers.115.self_attn.o_proj",
            "model.layers.115.self_attn.q_proj",
            "model.layers.115.self_attn.v_proj",
            "model.layers.116.self_attn.k_proj",
            "model.layers.116.self_attn.o_proj",
            "model.layers.116.self_attn.q_proj",
            "model.layers.116.self_attn.v_proj",
            "model.layers.117.self_attn.k_proj",
            "model.layers.117.self_attn.o_proj",
            "model.layers.117.self_attn.q_proj",
            "model.layers.117.self_attn.v_proj",
            "model.layers.118.self_attn.k_proj",
            "model.layers.118.self_attn.o_proj",
            "model.layers.118.self_attn.q_proj",
            "model.layers.118.self_attn.v_proj",
            "model.layers.119.self_attn.k_proj",
            "model.layers.119.self_attn.o_proj",
            "model.layers.119.self_attn.q_proj",
            "model.layers.119.self_attn.v_proj",
            "model.layers.120.self_attn.k_proj",
            "model.layers.120.self_attn.o_proj",
            "model.layers.120.self_attn.q_proj",
            "model.layers.120.self_attn.v_proj",
            "model.layers.121.self_attn.k_proj",
            "model.layers.121.self_attn.o_proj",
            "model.layers.121.self_attn.q_proj",
            "model.layers.121.self_attn.v_proj",
            "model.layers.122.self_attn.k_proj",
            "model.layers.122.self_attn.o_proj",
            "model.layers.122.self_attn.q_proj",
            "model.layers.122.self_attn.v_proj",
            "model.layers.123.self_attn.k_proj",
            "model.layers.123.self_attn.o_proj",
            "model.layers.123.self_attn.q_proj",
            "model.layers.123.self_attn.v_proj",
            "model.layers.124.self_attn.k_proj",
            "model.layers.124.self_attn.o_proj",
            "model.layers.124.self_attn.q_proj",
            "model.layers.124.self_attn.v_proj",
            "model.layers.125.self_attn.k_proj",
            "model.layers.125.self_attn.o_proj",
            "model.layers.125.self_attn.q_proj",
            "model.layers.125.self_attn.v_proj"
          ],
          "quant_method": "fbgemm_fp8"
        },
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-70B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-70B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-8B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-8B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3-70B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3-70B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128009,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "llava-hf/llava-v1.6-mistral-7b-hf":{
        "architectures": [
          "LlavaNextForConditionalGeneration"
        ],
        "ignore_index": -100,
        "image_grid_pinpoints": [
          [
            336,
            672
          ],
          [
            672,
            336
          ],
          [
            672,
            672
          ],
          [
            1008,
            336
          ],
          [
            336,
            1008
          ]
        ],
        "image_token_index": 32000,
        "model_type": "llava_next",
        "projector_hidden_act": "gelu",
        "text_config": {
          "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
          "architectures": [
            "MistralForCausalLM"
          ],
          "intermediate_size": 14336,
          "max_position_embeddings": 32768,
          "model_type": "mistral",
          "num_key_value_heads": 8,
          "rms_norm_eps": 1e-05,
          "rope_theta": 1000000.0,
          "sliding_window": null,
          "torch_dtype": "bfloat16",
          "vocab_size": 32064
        },
        "torch_dtype": "float16",
        "transformers_version": "4.39.0.dev0",
        "use_image_newline_parameter": true,
        "vision_config": {
          "hidden_size": 1024,
          "image_size": 336,
          "intermediate_size": 4096,
          "model_type": "clip_vision_model",
          "num_attention_heads": 16,
          "num_hidden_layers": 24,
          "patch_size": 14,
          "projection_dim": 768,
          "vocab_size": 32000
        },
        "vision_feature_layer": -2,
        "vision_feature_select_strategy": "default",
        "vocab_size": 32064
      },
      "mistralai/Mixtral-8x7B-Instruct-v0.1":{
        "architectures": [
          "MixtralForCausalLM"
        ],
        "attention_dropout": 0.0,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 32768,
        "model_type": "mixtral",
        "num_attention_heads": 32,
        "num_experts_per_tok": 2,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "num_local_experts": 8,
        "output_router_logits": false,
        "rms_norm_eps": 1e-05,
        "rope_theta": 1000000.0,
        "router_aux_loss_coef": 0.02,
        "sliding_window": null,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.36.0.dev0",
        "use_cache": true,
        "vocab_size": 32000
      },
      "mistralai/Mixtral-8x7B-v0.1":{
        "architectures": [
          "MixtralForCausalLM"
        ],
        "attention_dropout": 0.0,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 32768,
        "model_type": "mixtral",
        "num_attention_heads": 32,
        "num_experts_per_tok": 2,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "num_local_experts": 8,
        "output_router_logits": false,
        "rms_norm_eps": 1e-05,
        "rope_theta": 1000000.0,
        "router_aux_loss_coef": 0.02,
        "sliding_window": null,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.36.0.dev0",
        "use_cache": true,
        "vocab_size": 32000
      }
}