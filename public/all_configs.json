{
    "meta-llama/Meta-Llama-3-8B": {
        "architectures": [
        "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
    },
    "meta-llama/Meta-Llama-3-8B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128009,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Prompt-Guard-86M":{
        "_name_or_path": "/tmp/tmpvlbiibjx",
        "architectures": [
          "DebertaV2ForSequenceClassification"
        ],
        "attention_probs_dropout_prob": 0.1,
        "hidden_act": "gelu",
        "hidden_dropout_prob": 0.1,
        "hidden_size": 768,
        "id2label": {
          "0": "BENIGN",
          "1": "INJECTION",
          "2": "JAILBREAK"
        },
        "initializer_range": 0.02,
        "intermediate_size": 3072,
        "label2id": {
          "BENIGN": 0,
          "INJECTION": 1,
          "JAILBREAK": 2
        },
        "layer_norm_eps": 1e-07,
        "max_position_embeddings": 512,
        "max_relative_positions": -1,
        "model_type": "deberta-v2",
        "norm_rel_ebd": "layer_norm",
        "num_attention_heads": 12,
        "num_hidden_layers": 12,
        "pad_token_id": 0,
        "pooler_dropout": 0,
        "pooler_hidden_act": "gelu",
        "pooler_hidden_size": 768,
        "pos_att_type": [
          "p2c",
          "c2p"
        ],
        "position_biased_input": false,
        "position_buckets": 256,
        "relative_attention": true,
        "share_att_key": true,
        "torch_dtype": "float32",
        "transformers_version": "4.41.2",
        "type_vocab_size": 0,
        "vocab_size": 251000
      },
      "meta-llama/Llama-Guard-3-8B-INT8":{
        "_name_or_path": "",
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "quantization_config": {
          "_load_in_4bit": false,
          "_load_in_8bit": true,
          "bnb_4bit_compute_dtype": "float32",
          "bnb_4bit_quant_storage": "uint8",
          "bnb_4bit_quant_type": "fp4",
          "bnb_4bit_use_double_quant": false,
          "llm_int8_enable_fp32_cpu_offload": false,
          "llm_int8_has_fp16_weight": false,
          "llm_int8_skip_modules": null,
          "llm_int8_threshold": 6.0,
          "load_in_4bit": false,
          "load_in_8bit": true,
          "quant_method": "bitsandbytes"
        },
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Llama-Guard-3-8B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B-Instruct-FP8":{
        "_name_or_path": "meta-llama/Meta-Llama-3.1-405B-Instruct-FP8",
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "quantization_config": {
          "activation_scale_ub": 1200.0,
          "modules_to_not_convert": [
            "model.layers.0.mlp.down_proj",
            "model.layers.0.mlp.gate_proj",
            "model.layers.0.mlp.up_proj",
            "model.layers.125.mlp.down_proj",
            "model.layers.125.mlp.gate_proj",
            "model.layers.125.mlp.up_proj",
            "model.layers.0.self_attn.k_proj",
            "model.layers.0.self_attn.o_proj",
            "model.layers.0.self_attn.q_proj",
            "model.layers.0.self_attn.v_proj",
            "model.layers.1.self_attn.k_proj",
            "model.layers.1.self_attn.o_proj",
            "model.layers.1.self_attn.q_proj",
            "model.layers.1.self_attn.v_proj",
            "model.layers.2.self_attn.k_proj",
            "model.layers.2.self_attn.o_proj",
            "model.layers.2.self_attn.q_proj",
            "model.layers.2.self_attn.v_proj",
            "model.layers.3.self_attn.k_proj",
            "model.layers.3.self_attn.o_proj",
            "model.layers.3.self_attn.q_proj",
            "model.layers.3.self_attn.v_proj",
            "model.layers.4.self_attn.k_proj",
            "model.layers.4.self_attn.o_proj",
            "model.layers.4.self_attn.q_proj",
            "model.layers.4.self_attn.v_proj",
            "model.layers.5.self_attn.k_proj",
            "model.layers.5.self_attn.o_proj",
            "model.layers.5.self_attn.q_proj",
            "model.layers.5.self_attn.v_proj",
            "model.layers.6.self_attn.k_proj",
            "model.layers.6.self_attn.o_proj",
            "model.layers.6.self_attn.q_proj",
            "model.layers.6.self_attn.v_proj",
            "model.layers.7.self_attn.k_proj",
            "model.layers.7.self_attn.o_proj",
            "model.layers.7.self_attn.q_proj",
            "model.layers.7.self_attn.v_proj",
            "model.layers.8.self_attn.k_proj",
            "model.layers.8.self_attn.o_proj",
            "model.layers.8.self_attn.q_proj",
            "model.layers.8.self_attn.v_proj",
            "model.layers.9.self_attn.k_proj",
            "model.layers.9.self_attn.o_proj",
            "model.layers.9.self_attn.q_proj",
            "model.layers.9.self_attn.v_proj",
            "model.layers.10.self_attn.k_proj",
            "model.layers.10.self_attn.o_proj",
            "model.layers.10.self_attn.q_proj",
            "model.layers.10.self_attn.v_proj",
            "model.layers.11.self_attn.k_proj",
            "model.layers.11.self_attn.o_proj",
            "model.layers.11.self_attn.q_proj",
            "model.layers.11.self_attn.v_proj",
            "model.layers.12.self_attn.k_proj",
            "model.layers.12.self_attn.o_proj",
            "model.layers.12.self_attn.q_proj",
            "model.layers.12.self_attn.v_proj",
            "model.layers.13.self_attn.k_proj",
            "model.layers.13.self_attn.o_proj",
            "model.layers.13.self_attn.q_proj",
            "model.layers.13.self_attn.v_proj",
            "model.layers.14.self_attn.k_proj",
            "model.layers.14.self_attn.o_proj",
            "model.layers.14.self_attn.q_proj",
            "model.layers.14.self_attn.v_proj",
            "model.layers.15.self_attn.k_proj",
            "model.layers.15.self_attn.o_proj",
            "model.layers.15.self_attn.q_proj",
            "model.layers.15.self_attn.v_proj",
            "model.layers.16.self_attn.k_proj",
            "model.layers.16.self_attn.o_proj",
            "model.layers.16.self_attn.q_proj",
            "model.layers.16.self_attn.v_proj",
            "model.layers.17.self_attn.k_proj",
            "model.layers.17.self_attn.o_proj",
            "model.layers.17.self_attn.q_proj",
            "model.layers.17.self_attn.v_proj",
            "model.layers.18.self_attn.k_proj",
            "model.layers.18.self_attn.o_proj",
            "model.layers.18.self_attn.q_proj",
            "model.layers.18.self_attn.v_proj",
            "model.layers.19.self_attn.k_proj",
            "model.layers.19.self_attn.o_proj",
            "model.layers.19.self_attn.q_proj",
            "model.layers.19.self_attn.v_proj",
            "model.layers.20.self_attn.k_proj",
            "model.layers.20.self_attn.o_proj",
            "model.layers.20.self_attn.q_proj",
            "model.layers.20.self_attn.v_proj",
            "model.layers.21.self_attn.k_proj",
            "model.layers.21.self_attn.o_proj",
            "model.layers.21.self_attn.q_proj",
            "model.layers.21.self_attn.v_proj",
            "model.layers.22.self_attn.k_proj",
            "model.layers.22.self_attn.o_proj",
            "model.layers.22.self_attn.q_proj",
            "model.layers.22.self_attn.v_proj",
            "model.layers.23.self_attn.k_proj",
            "model.layers.23.self_attn.o_proj",
            "model.layers.23.self_attn.q_proj",
            "model.layers.23.self_attn.v_proj",
            "model.layers.24.self_attn.k_proj",
            "model.layers.24.self_attn.o_proj",
            "model.layers.24.self_attn.q_proj",
            "model.layers.24.self_attn.v_proj",
            "model.layers.25.self_attn.k_proj",
            "model.layers.25.self_attn.o_proj",
            "model.layers.25.self_attn.q_proj",
            "model.layers.25.self_attn.v_proj",
            "model.layers.26.self_attn.k_proj",
            "model.layers.26.self_attn.o_proj",
            "model.layers.26.self_attn.q_proj",
            "model.layers.26.self_attn.v_proj",
            "model.layers.27.self_attn.k_proj",
            "model.layers.27.self_attn.o_proj",
            "model.layers.27.self_attn.q_proj",
            "model.layers.27.self_attn.v_proj",
            "model.layers.28.self_attn.k_proj",
            "model.layers.28.self_attn.o_proj",
            "model.layers.28.self_attn.q_proj",
            "model.layers.28.self_attn.v_proj",
            "model.layers.29.self_attn.k_proj",
            "model.layers.29.self_attn.o_proj",
            "model.layers.29.self_attn.q_proj",
            "model.layers.29.self_attn.v_proj",
            "model.layers.30.self_attn.k_proj",
            "model.layers.30.self_attn.o_proj",
            "model.layers.30.self_attn.q_proj",
            "model.layers.30.self_attn.v_proj",
            "model.layers.31.self_attn.k_proj",
            "model.layers.31.self_attn.o_proj",
            "model.layers.31.self_attn.q_proj",
            "model.layers.31.self_attn.v_proj",
            "model.layers.32.self_attn.k_proj",
            "model.layers.32.self_attn.o_proj",
            "model.layers.32.self_attn.q_proj",
            "model.layers.32.self_attn.v_proj",
            "model.layers.33.self_attn.k_proj",
            "model.layers.33.self_attn.o_proj",
            "model.layers.33.self_attn.q_proj",
            "model.layers.33.self_attn.v_proj",
            "model.layers.34.self_attn.k_proj",
            "model.layers.34.self_attn.o_proj",
            "model.layers.34.self_attn.q_proj",
            "model.layers.34.self_attn.v_proj",
            "model.layers.35.self_attn.k_proj",
            "model.layers.35.self_attn.o_proj",
            "model.layers.35.self_attn.q_proj",
            "model.layers.35.self_attn.v_proj",
            "model.layers.36.self_attn.k_proj",
            "model.layers.36.self_attn.o_proj",
            "model.layers.36.self_attn.q_proj",
            "model.layers.36.self_attn.v_proj",
            "model.layers.37.self_attn.k_proj",
            "model.layers.37.self_attn.o_proj",
            "model.layers.37.self_attn.q_proj",
            "model.layers.37.self_attn.v_proj",
            "model.layers.38.self_attn.k_proj",
            "model.layers.38.self_attn.o_proj",
            "model.layers.38.self_attn.q_proj",
            "model.layers.38.self_attn.v_proj",
            "model.layers.39.self_attn.k_proj",
            "model.layers.39.self_attn.o_proj",
            "model.layers.39.self_attn.q_proj",
            "model.layers.39.self_attn.v_proj",
            "model.layers.40.self_attn.k_proj",
            "model.layers.40.self_attn.o_proj",
            "model.layers.40.self_attn.q_proj",
            "model.layers.40.self_attn.v_proj",
            "model.layers.41.self_attn.k_proj",
            "model.layers.41.self_attn.o_proj",
            "model.layers.41.self_attn.q_proj",
            "model.layers.41.self_attn.v_proj",
            "model.layers.42.self_attn.k_proj",
            "model.layers.42.self_attn.o_proj",
            "model.layers.42.self_attn.q_proj",
            "model.layers.42.self_attn.v_proj",
            "model.layers.43.self_attn.k_proj",
            "model.layers.43.self_attn.o_proj",
            "model.layers.43.self_attn.q_proj",
            "model.layers.43.self_attn.v_proj",
            "model.layers.44.self_attn.k_proj",
            "model.layers.44.self_attn.o_proj",
            "model.layers.44.self_attn.q_proj",
            "model.layers.44.self_attn.v_proj",
            "model.layers.45.self_attn.k_proj",
            "model.layers.45.self_attn.o_proj",
            "model.layers.45.self_attn.q_proj",
            "model.layers.45.self_attn.v_proj",
            "model.layers.46.self_attn.k_proj",
            "model.layers.46.self_attn.o_proj",
            "model.layers.46.self_attn.q_proj",
            "model.layers.46.self_attn.v_proj",
            "model.layers.47.self_attn.k_proj",
            "model.layers.47.self_attn.o_proj",
            "model.layers.47.self_attn.q_proj",
            "model.layers.47.self_attn.v_proj",
            "model.layers.48.self_attn.k_proj",
            "model.layers.48.self_attn.o_proj",
            "model.layers.48.self_attn.q_proj",
            "model.layers.48.self_attn.v_proj",
            "model.layers.49.self_attn.k_proj",
            "model.layers.49.self_attn.o_proj",
            "model.layers.49.self_attn.q_proj",
            "model.layers.49.self_attn.v_proj",
            "model.layers.50.self_attn.k_proj",
            "model.layers.50.self_attn.o_proj",
            "model.layers.50.self_attn.q_proj",
            "model.layers.50.self_attn.v_proj",
            "model.layers.51.self_attn.k_proj",
            "model.layers.51.self_attn.o_proj",
            "model.layers.51.self_attn.q_proj",
            "model.layers.51.self_attn.v_proj",
            "model.layers.52.self_attn.k_proj",
            "model.layers.52.self_attn.o_proj",
            "model.layers.52.self_attn.q_proj",
            "model.layers.52.self_attn.v_proj",
            "model.layers.53.self_attn.k_proj",
            "model.layers.53.self_attn.o_proj",
            "model.layers.53.self_attn.q_proj",
            "model.layers.53.self_attn.v_proj",
            "model.layers.54.self_attn.k_proj",
            "model.layers.54.self_attn.o_proj",
            "model.layers.54.self_attn.q_proj",
            "model.layers.54.self_attn.v_proj",
            "model.layers.55.self_attn.k_proj",
            "model.layers.55.self_attn.o_proj",
            "model.layers.55.self_attn.q_proj",
            "model.layers.55.self_attn.v_proj",
            "model.layers.56.self_attn.k_proj",
            "model.layers.56.self_attn.o_proj",
            "model.layers.56.self_attn.q_proj",
            "model.layers.56.self_attn.v_proj",
            "model.layers.57.self_attn.k_proj",
            "model.layers.57.self_attn.o_proj",
            "model.layers.57.self_attn.q_proj",
            "model.layers.57.self_attn.v_proj",
            "model.layers.58.self_attn.k_proj",
            "model.layers.58.self_attn.o_proj",
            "model.layers.58.self_attn.q_proj",
            "model.layers.58.self_attn.v_proj",
            "model.layers.59.self_attn.k_proj",
            "model.layers.59.self_attn.o_proj",
            "model.layers.59.self_attn.q_proj",
            "model.layers.59.self_attn.v_proj",
            "model.layers.60.self_attn.k_proj",
            "model.layers.60.self_attn.o_proj",
            "model.layers.60.self_attn.q_proj",
            "model.layers.60.self_attn.v_proj",
            "model.layers.61.self_attn.k_proj",
            "model.layers.61.self_attn.o_proj",
            "model.layers.61.self_attn.q_proj",
            "model.layers.61.self_attn.v_proj",
            "model.layers.62.self_attn.k_proj",
            "model.layers.62.self_attn.o_proj",
            "model.layers.62.self_attn.q_proj",
            "model.layers.62.self_attn.v_proj",
            "model.layers.63.self_attn.k_proj",
            "model.layers.63.self_attn.o_proj",
            "model.layers.63.self_attn.q_proj",
            "model.layers.63.self_attn.v_proj",
            "model.layers.64.self_attn.k_proj",
            "model.layers.64.self_attn.o_proj",
            "model.layers.64.self_attn.q_proj",
            "model.layers.64.self_attn.v_proj",
            "model.layers.65.self_attn.k_proj",
            "model.layers.65.self_attn.o_proj",
            "model.layers.65.self_attn.q_proj",
            "model.layers.65.self_attn.v_proj",
            "model.layers.66.self_attn.k_proj",
            "model.layers.66.self_attn.o_proj",
            "model.layers.66.self_attn.q_proj",
            "model.layers.66.self_attn.v_proj",
            "model.layers.67.self_attn.k_proj",
            "model.layers.67.self_attn.o_proj",
            "model.layers.67.self_attn.q_proj",
            "model.layers.67.self_attn.v_proj",
            "model.layers.68.self_attn.k_proj",
            "model.layers.68.self_attn.o_proj",
            "model.layers.68.self_attn.q_proj",
            "model.layers.68.self_attn.v_proj",
            "model.layers.69.self_attn.k_proj",
            "model.layers.69.self_attn.o_proj",
            "model.layers.69.self_attn.q_proj",
            "model.layers.69.self_attn.v_proj",
            "model.layers.70.self_attn.k_proj",
            "model.layers.70.self_attn.o_proj",
            "model.layers.70.self_attn.q_proj",
            "model.layers.70.self_attn.v_proj",
            "model.layers.71.self_attn.k_proj",
            "model.layers.71.self_attn.o_proj",
            "model.layers.71.self_attn.q_proj",
            "model.layers.71.self_attn.v_proj",
            "model.layers.72.self_attn.k_proj",
            "model.layers.72.self_attn.o_proj",
            "model.layers.72.self_attn.q_proj",
            "model.layers.72.self_attn.v_proj",
            "model.layers.73.self_attn.k_proj",
            "model.layers.73.self_attn.o_proj",
            "model.layers.73.self_attn.q_proj",
            "model.layers.73.self_attn.v_proj",
            "model.layers.74.self_attn.k_proj",
            "model.layers.74.self_attn.o_proj",
            "model.layers.74.self_attn.q_proj",
            "model.layers.74.self_attn.v_proj",
            "model.layers.75.self_attn.k_proj",
            "model.layers.75.self_attn.o_proj",
            "model.layers.75.self_attn.q_proj",
            "model.layers.75.self_attn.v_proj",
            "model.layers.76.self_attn.k_proj",
            "model.layers.76.self_attn.o_proj",
            "model.layers.76.self_attn.q_proj",
            "model.layers.76.self_attn.v_proj",
            "model.layers.77.self_attn.k_proj",
            "model.layers.77.self_attn.o_proj",
            "model.layers.77.self_attn.q_proj",
            "model.layers.77.self_attn.v_proj",
            "model.layers.78.self_attn.k_proj",
            "model.layers.78.self_attn.o_proj",
            "model.layers.78.self_attn.q_proj",
            "model.layers.78.self_attn.v_proj",
            "model.layers.79.self_attn.k_proj",
            "model.layers.79.self_attn.o_proj",
            "model.layers.79.self_attn.q_proj",
            "model.layers.79.self_attn.v_proj",
            "model.layers.80.self_attn.k_proj",
            "model.layers.80.self_attn.o_proj",
            "model.layers.80.self_attn.q_proj",
            "model.layers.80.self_attn.v_proj",
            "model.layers.81.self_attn.k_proj",
            "model.layers.81.self_attn.o_proj",
            "model.layers.81.self_attn.q_proj",
            "model.layers.81.self_attn.v_proj",
            "model.layers.82.self_attn.k_proj",
            "model.layers.82.self_attn.o_proj",
            "model.layers.82.self_attn.q_proj",
            "model.layers.82.self_attn.v_proj",
            "model.layers.83.self_attn.k_proj",
            "model.layers.83.self_attn.o_proj",
            "model.layers.83.self_attn.q_proj",
            "model.layers.83.self_attn.v_proj",
            "model.layers.84.self_attn.k_proj",
            "model.layers.84.self_attn.o_proj",
            "model.layers.84.self_attn.q_proj",
            "model.layers.84.self_attn.v_proj",
            "model.layers.85.self_attn.k_proj",
            "model.layers.85.self_attn.o_proj",
            "model.layers.85.self_attn.q_proj",
            "model.layers.85.self_attn.v_proj",
            "model.layers.86.self_attn.k_proj",
            "model.layers.86.self_attn.o_proj",
            "model.layers.86.self_attn.q_proj",
            "model.layers.86.self_attn.v_proj",
            "model.layers.87.self_attn.k_proj",
            "model.layers.87.self_attn.o_proj",
            "model.layers.87.self_attn.q_proj",
            "model.layers.87.self_attn.v_proj",
            "model.layers.88.self_attn.k_proj",
            "model.layers.88.self_attn.o_proj",
            "model.layers.88.self_attn.q_proj",
            "model.layers.88.self_attn.v_proj",
            "model.layers.89.self_attn.k_proj",
            "model.layers.89.self_attn.o_proj",
            "model.layers.89.self_attn.q_proj",
            "model.layers.89.self_attn.v_proj",
            "model.layers.90.self_attn.k_proj",
            "model.layers.90.self_attn.o_proj",
            "model.layers.90.self_attn.q_proj",
            "model.layers.90.self_attn.v_proj",
            "model.layers.91.self_attn.k_proj",
            "model.layers.91.self_attn.o_proj",
            "model.layers.91.self_attn.q_proj",
            "model.layers.91.self_attn.v_proj",
            "model.layers.92.self_attn.k_proj",
            "model.layers.92.self_attn.o_proj",
            "model.layers.92.self_attn.q_proj",
            "model.layers.92.self_attn.v_proj",
            "model.layers.93.self_attn.k_proj",
            "model.layers.93.self_attn.o_proj",
            "model.layers.93.self_attn.q_proj",
            "model.layers.93.self_attn.v_proj",
            "model.layers.94.self_attn.k_proj",
            "model.layers.94.self_attn.o_proj",
            "model.layers.94.self_attn.q_proj",
            "model.layers.94.self_attn.v_proj",
            "model.layers.95.self_attn.k_proj",
            "model.layers.95.self_attn.o_proj",
            "model.layers.95.self_attn.q_proj",
            "model.layers.95.self_attn.v_proj",
            "model.layers.96.self_attn.k_proj",
            "model.layers.96.self_attn.o_proj",
            "model.layers.96.self_attn.q_proj",
            "model.layers.96.self_attn.v_proj",
            "model.layers.97.self_attn.k_proj",
            "model.layers.97.self_attn.o_proj",
            "model.layers.97.self_attn.q_proj",
            "model.layers.97.self_attn.v_proj",
            "model.layers.98.self_attn.k_proj",
            "model.layers.98.self_attn.o_proj",
            "model.layers.98.self_attn.q_proj",
            "model.layers.98.self_attn.v_proj",
            "model.layers.99.self_attn.k_proj",
            "model.layers.99.self_attn.o_proj",
            "model.layers.99.self_attn.q_proj",
            "model.layers.99.self_attn.v_proj",
            "model.layers.100.self_attn.k_proj",
            "model.layers.100.self_attn.o_proj",
            "model.layers.100.self_attn.q_proj",
            "model.layers.100.self_attn.v_proj",
            "model.layers.101.self_attn.k_proj",
            "model.layers.101.self_attn.o_proj",
            "model.layers.101.self_attn.q_proj",
            "model.layers.101.self_attn.v_proj",
            "model.layers.102.self_attn.k_proj",
            "model.layers.102.self_attn.o_proj",
            "model.layers.102.self_attn.q_proj",
            "model.layers.102.self_attn.v_proj",
            "model.layers.103.self_attn.k_proj",
            "model.layers.103.self_attn.o_proj",
            "model.layers.103.self_attn.q_proj",
            "model.layers.103.self_attn.v_proj",
            "model.layers.104.self_attn.k_proj",
            "model.layers.104.self_attn.o_proj",
            "model.layers.104.self_attn.q_proj",
            "model.layers.104.self_attn.v_proj",
            "model.layers.105.self_attn.k_proj",
            "model.layers.105.self_attn.o_proj",
            "model.layers.105.self_attn.q_proj",
            "model.layers.105.self_attn.v_proj",
            "model.layers.106.self_attn.k_proj",
            "model.layers.106.self_attn.o_proj",
            "model.layers.106.self_attn.q_proj",
            "model.layers.106.self_attn.v_proj",
            "model.layers.107.self_attn.k_proj",
            "model.layers.107.self_attn.o_proj",
            "model.layers.107.self_attn.q_proj",
            "model.layers.107.self_attn.v_proj",
            "model.layers.108.self_attn.k_proj",
            "model.layers.108.self_attn.o_proj",
            "model.layers.108.self_attn.q_proj",
            "model.layers.108.self_attn.v_proj",
            "model.layers.109.self_attn.k_proj",
            "model.layers.109.self_attn.o_proj",
            "model.layers.109.self_attn.q_proj",
            "model.layers.109.self_attn.v_proj",
            "model.layers.110.self_attn.k_proj",
            "model.layers.110.self_attn.o_proj",
            "model.layers.110.self_attn.q_proj",
            "model.layers.110.self_attn.v_proj",
            "model.layers.111.self_attn.k_proj",
            "model.layers.111.self_attn.o_proj",
            "model.layers.111.self_attn.q_proj",
            "model.layers.111.self_attn.v_proj",
            "model.layers.112.self_attn.k_proj",
            "model.layers.112.self_attn.o_proj",
            "model.layers.112.self_attn.q_proj",
            "model.layers.112.self_attn.v_proj",
            "model.layers.113.self_attn.k_proj",
            "model.layers.113.self_attn.o_proj",
            "model.layers.113.self_attn.q_proj",
            "model.layers.113.self_attn.v_proj",
            "model.layers.114.self_attn.k_proj",
            "model.layers.114.self_attn.o_proj",
            "model.layers.114.self_attn.q_proj",
            "model.layers.114.self_attn.v_proj",
            "model.layers.115.self_attn.k_proj",
            "model.layers.115.self_attn.o_proj",
            "model.layers.115.self_attn.q_proj",
            "model.layers.115.self_attn.v_proj",
            "model.layers.116.self_attn.k_proj",
            "model.layers.116.self_attn.o_proj",
            "model.layers.116.self_attn.q_proj",
            "model.layers.116.self_attn.v_proj",
            "model.layers.117.self_attn.k_proj",
            "model.layers.117.self_attn.o_proj",
            "model.layers.117.self_attn.q_proj",
            "model.layers.117.self_attn.v_proj",
            "model.layers.118.self_attn.k_proj",
            "model.layers.118.self_attn.o_proj",
            "model.layers.118.self_attn.q_proj",
            "model.layers.118.self_attn.v_proj",
            "model.layers.119.self_attn.k_proj",
            "model.layers.119.self_attn.o_proj",
            "model.layers.119.self_attn.q_proj",
            "model.layers.119.self_attn.v_proj",
            "model.layers.120.self_attn.k_proj",
            "model.layers.120.self_attn.o_proj",
            "model.layers.120.self_attn.q_proj",
            "model.layers.120.self_attn.v_proj",
            "model.layers.121.self_attn.k_proj",
            "model.layers.121.self_attn.o_proj",
            "model.layers.121.self_attn.q_proj",
            "model.layers.121.self_attn.v_proj",
            "model.layers.122.self_attn.k_proj",
            "model.layers.122.self_attn.o_proj",
            "model.layers.122.self_attn.q_proj",
            "model.layers.122.self_attn.v_proj",
            "model.layers.123.self_attn.k_proj",
            "model.layers.123.self_attn.o_proj",
            "model.layers.123.self_attn.q_proj",
            "model.layers.123.self_attn.v_proj",
            "model.layers.124.self_attn.k_proj",
            "model.layers.124.self_attn.o_proj",
            "model.layers.124.self_attn.q_proj",
            "model.layers.124.self_attn.v_proj",
            "model.layers.125.self_attn.k_proj",
            "model.layers.125.self_attn.o_proj",
            "model.layers.125.self_attn.q_proj",
            "model.layers.125.self_attn.v_proj"
          ],
          "quant_method": "fbgemm_fp8"
        },
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B-FP8":{
        "_name_or_path": "meta-llama/Meta-Llama-3.1-405B-FP8",
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "quantization_config": {
          "activation_scale_ub": 1200.0,
          "modules_to_not_convert": [
            "lm_head",
            "model.layers.0.mlp.down_proj",
            "model.layers.0.mlp.gate_proj",
            "model.layers.0.mlp.up_proj",
            "model.layers.125.mlp.down_proj",
            "model.layers.125.mlp.gate_proj",
            "model.layers.125.mlp.up_proj",
            "model.layers.0.self_attn.k_proj",
            "model.layers.0.self_attn.o_proj",
            "model.layers.0.self_attn.q_proj",
            "model.layers.0.self_attn.v_proj",
            "model.layers.1.self_attn.k_proj",
            "model.layers.1.self_attn.o_proj",
            "model.layers.1.self_attn.q_proj",
            "model.layers.1.self_attn.v_proj",
            "model.layers.2.self_attn.k_proj",
            "model.layers.2.self_attn.o_proj",
            "model.layers.2.self_attn.q_proj",
            "model.layers.2.self_attn.v_proj",
            "model.layers.3.self_attn.k_proj",
            "model.layers.3.self_attn.o_proj",
            "model.layers.3.self_attn.q_proj",
            "model.layers.3.self_attn.v_proj",
            "model.layers.4.self_attn.k_proj",
            "model.layers.4.self_attn.o_proj",
            "model.layers.4.self_attn.q_proj",
            "model.layers.4.self_attn.v_proj",
            "model.layers.5.self_attn.k_proj",
            "model.layers.5.self_attn.o_proj",
            "model.layers.5.self_attn.q_proj",
            "model.layers.5.self_attn.v_proj",
            "model.layers.6.self_attn.k_proj",
            "model.layers.6.self_attn.o_proj",
            "model.layers.6.self_attn.q_proj",
            "model.layers.6.self_attn.v_proj",
            "model.layers.7.self_attn.k_proj",
            "model.layers.7.self_attn.o_proj",
            "model.layers.7.self_attn.q_proj",
            "model.layers.7.self_attn.v_proj",
            "model.layers.8.self_attn.k_proj",
            "model.layers.8.self_attn.o_proj",
            "model.layers.8.self_attn.q_proj",
            "model.layers.8.self_attn.v_proj",
            "model.layers.9.self_attn.k_proj",
            "model.layers.9.self_attn.o_proj",
            "model.layers.9.self_attn.q_proj",
            "model.layers.9.self_attn.v_proj",
            "model.layers.10.self_attn.k_proj",
            "model.layers.10.self_attn.o_proj",
            "model.layers.10.self_attn.q_proj",
            "model.layers.10.self_attn.v_proj",
            "model.layers.11.self_attn.k_proj",
            "model.layers.11.self_attn.o_proj",
            "model.layers.11.self_attn.q_proj",
            "model.layers.11.self_attn.v_proj",
            "model.layers.12.self_attn.k_proj",
            "model.layers.12.self_attn.o_proj",
            "model.layers.12.self_attn.q_proj",
            "model.layers.12.self_attn.v_proj",
            "model.layers.13.self_attn.k_proj",
            "model.layers.13.self_attn.o_proj",
            "model.layers.13.self_attn.q_proj",
            "model.layers.13.self_attn.v_proj",
            "model.layers.14.self_attn.k_proj",
            "model.layers.14.self_attn.o_proj",
            "model.layers.14.self_attn.q_proj",
            "model.layers.14.self_attn.v_proj",
            "model.layers.15.self_attn.k_proj",
            "model.layers.15.self_attn.o_proj",
            "model.layers.15.self_attn.q_proj",
            "model.layers.15.self_attn.v_proj",
            "model.layers.16.self_attn.k_proj",
            "model.layers.16.self_attn.o_proj",
            "model.layers.16.self_attn.q_proj",
            "model.layers.16.self_attn.v_proj",
            "model.layers.17.self_attn.k_proj",
            "model.layers.17.self_attn.o_proj",
            "model.layers.17.self_attn.q_proj",
            "model.layers.17.self_attn.v_proj",
            "model.layers.18.self_attn.k_proj",
            "model.layers.18.self_attn.o_proj",
            "model.layers.18.self_attn.q_proj",
            "model.layers.18.self_attn.v_proj",
            "model.layers.19.self_attn.k_proj",
            "model.layers.19.self_attn.o_proj",
            "model.layers.19.self_attn.q_proj",
            "model.layers.19.self_attn.v_proj",
            "model.layers.20.self_attn.k_proj",
            "model.layers.20.self_attn.o_proj",
            "model.layers.20.self_attn.q_proj",
            "model.layers.20.self_attn.v_proj",
            "model.layers.21.self_attn.k_proj",
            "model.layers.21.self_attn.o_proj",
            "model.layers.21.self_attn.q_proj",
            "model.layers.21.self_attn.v_proj",
            "model.layers.22.self_attn.k_proj",
            "model.layers.22.self_attn.o_proj",
            "model.layers.22.self_attn.q_proj",
            "model.layers.22.self_attn.v_proj",
            "model.layers.23.self_attn.k_proj",
            "model.layers.23.self_attn.o_proj",
            "model.layers.23.self_attn.q_proj",
            "model.layers.23.self_attn.v_proj",
            "model.layers.24.self_attn.k_proj",
            "model.layers.24.self_attn.o_proj",
            "model.layers.24.self_attn.q_proj",
            "model.layers.24.self_attn.v_proj",
            "model.layers.25.self_attn.k_proj",
            "model.layers.25.self_attn.o_proj",
            "model.layers.25.self_attn.q_proj",
            "model.layers.25.self_attn.v_proj",
            "model.layers.26.self_attn.k_proj",
            "model.layers.26.self_attn.o_proj",
            "model.layers.26.self_attn.q_proj",
            "model.layers.26.self_attn.v_proj",
            "model.layers.27.self_attn.k_proj",
            "model.layers.27.self_attn.o_proj",
            "model.layers.27.self_attn.q_proj",
            "model.layers.27.self_attn.v_proj",
            "model.layers.28.self_attn.k_proj",
            "model.layers.28.self_attn.o_proj",
            "model.layers.28.self_attn.q_proj",
            "model.layers.28.self_attn.v_proj",
            "model.layers.29.self_attn.k_proj",
            "model.layers.29.self_attn.o_proj",
            "model.layers.29.self_attn.q_proj",
            "model.layers.29.self_attn.v_proj",
            "model.layers.30.self_attn.k_proj",
            "model.layers.30.self_attn.o_proj",
            "model.layers.30.self_attn.q_proj",
            "model.layers.30.self_attn.v_proj",
            "model.layers.31.self_attn.k_proj",
            "model.layers.31.self_attn.o_proj",
            "model.layers.31.self_attn.q_proj",
            "model.layers.31.self_attn.v_proj",
            "model.layers.32.self_attn.k_proj",
            "model.layers.32.self_attn.o_proj",
            "model.layers.32.self_attn.q_proj",
            "model.layers.32.self_attn.v_proj",
            "model.layers.33.self_attn.k_proj",
            "model.layers.33.self_attn.o_proj",
            "model.layers.33.self_attn.q_proj",
            "model.layers.33.self_attn.v_proj",
            "model.layers.34.self_attn.k_proj",
            "model.layers.34.self_attn.o_proj",
            "model.layers.34.self_attn.q_proj",
            "model.layers.34.self_attn.v_proj",
            "model.layers.35.self_attn.k_proj",
            "model.layers.35.self_attn.o_proj",
            "model.layers.35.self_attn.q_proj",
            "model.layers.35.self_attn.v_proj",
            "model.layers.36.self_attn.k_proj",
            "model.layers.36.self_attn.o_proj",
            "model.layers.36.self_attn.q_proj",
            "model.layers.36.self_attn.v_proj",
            "model.layers.37.self_attn.k_proj",
            "model.layers.37.self_attn.o_proj",
            "model.layers.37.self_attn.q_proj",
            "model.layers.37.self_attn.v_proj",
            "model.layers.38.self_attn.k_proj",
            "model.layers.38.self_attn.o_proj",
            "model.layers.38.self_attn.q_proj",
            "model.layers.38.self_attn.v_proj",
            "model.layers.39.self_attn.k_proj",
            "model.layers.39.self_attn.o_proj",
            "model.layers.39.self_attn.q_proj",
            "model.layers.39.self_attn.v_proj",
            "model.layers.40.self_attn.k_proj",
            "model.layers.40.self_attn.o_proj",
            "model.layers.40.self_attn.q_proj",
            "model.layers.40.self_attn.v_proj",
            "model.layers.41.self_attn.k_proj",
            "model.layers.41.self_attn.o_proj",
            "model.layers.41.self_attn.q_proj",
            "model.layers.41.self_attn.v_proj",
            "model.layers.42.self_attn.k_proj",
            "model.layers.42.self_attn.o_proj",
            "model.layers.42.self_attn.q_proj",
            "model.layers.42.self_attn.v_proj",
            "model.layers.43.self_attn.k_proj",
            "model.layers.43.self_attn.o_proj",
            "model.layers.43.self_attn.q_proj",
            "model.layers.43.self_attn.v_proj",
            "model.layers.44.self_attn.k_proj",
            "model.layers.44.self_attn.o_proj",
            "model.layers.44.self_attn.q_proj",
            "model.layers.44.self_attn.v_proj",
            "model.layers.45.self_attn.k_proj",
            "model.layers.45.self_attn.o_proj",
            "model.layers.45.self_attn.q_proj",
            "model.layers.45.self_attn.v_proj",
            "model.layers.46.self_attn.k_proj",
            "model.layers.46.self_attn.o_proj",
            "model.layers.46.self_attn.q_proj",
            "model.layers.46.self_attn.v_proj",
            "model.layers.47.self_attn.k_proj",
            "model.layers.47.self_attn.o_proj",
            "model.layers.47.self_attn.q_proj",
            "model.layers.47.self_attn.v_proj",
            "model.layers.48.self_attn.k_proj",
            "model.layers.48.self_attn.o_proj",
            "model.layers.48.self_attn.q_proj",
            "model.layers.48.self_attn.v_proj",
            "model.layers.49.self_attn.k_proj",
            "model.layers.49.self_attn.o_proj",
            "model.layers.49.self_attn.q_proj",
            "model.layers.49.self_attn.v_proj",
            "model.layers.50.self_attn.k_proj",
            "model.layers.50.self_attn.o_proj",
            "model.layers.50.self_attn.q_proj",
            "model.layers.50.self_attn.v_proj",
            "model.layers.51.self_attn.k_proj",
            "model.layers.51.self_attn.o_proj",
            "model.layers.51.self_attn.q_proj",
            "model.layers.51.self_attn.v_proj",
            "model.layers.52.self_attn.k_proj",
            "model.layers.52.self_attn.o_proj",
            "model.layers.52.self_attn.q_proj",
            "model.layers.52.self_attn.v_proj",
            "model.layers.53.self_attn.k_proj",
            "model.layers.53.self_attn.o_proj",
            "model.layers.53.self_attn.q_proj",
            "model.layers.53.self_attn.v_proj",
            "model.layers.54.self_attn.k_proj",
            "model.layers.54.self_attn.o_proj",
            "model.layers.54.self_attn.q_proj",
            "model.layers.54.self_attn.v_proj",
            "model.layers.55.self_attn.k_proj",
            "model.layers.55.self_attn.o_proj",
            "model.layers.55.self_attn.q_proj",
            "model.layers.55.self_attn.v_proj",
            "model.layers.56.self_attn.k_proj",
            "model.layers.56.self_attn.o_proj",
            "model.layers.56.self_attn.q_proj",
            "model.layers.56.self_attn.v_proj",
            "model.layers.57.self_attn.k_proj",
            "model.layers.57.self_attn.o_proj",
            "model.layers.57.self_attn.q_proj",
            "model.layers.57.self_attn.v_proj",
            "model.layers.58.self_attn.k_proj",
            "model.layers.58.self_attn.o_proj",
            "model.layers.58.self_attn.q_proj",
            "model.layers.58.self_attn.v_proj",
            "model.layers.59.self_attn.k_proj",
            "model.layers.59.self_attn.o_proj",
            "model.layers.59.self_attn.q_proj",
            "model.layers.59.self_attn.v_proj",
            "model.layers.60.self_attn.k_proj",
            "model.layers.60.self_attn.o_proj",
            "model.layers.60.self_attn.q_proj",
            "model.layers.60.self_attn.v_proj",
            "model.layers.61.self_attn.k_proj",
            "model.layers.61.self_attn.o_proj",
            "model.layers.61.self_attn.q_proj",
            "model.layers.61.self_attn.v_proj",
            "model.layers.62.self_attn.k_proj",
            "model.layers.62.self_attn.o_proj",
            "model.layers.62.self_attn.q_proj",
            "model.layers.62.self_attn.v_proj",
            "model.layers.63.self_attn.k_proj",
            "model.layers.63.self_attn.o_proj",
            "model.layers.63.self_attn.q_proj",
            "model.layers.63.self_attn.v_proj",
            "model.layers.64.self_attn.k_proj",
            "model.layers.64.self_attn.o_proj",
            "model.layers.64.self_attn.q_proj",
            "model.layers.64.self_attn.v_proj",
            "model.layers.65.self_attn.k_proj",
            "model.layers.65.self_attn.o_proj",
            "model.layers.65.self_attn.q_proj",
            "model.layers.65.self_attn.v_proj",
            "model.layers.66.self_attn.k_proj",
            "model.layers.66.self_attn.o_proj",
            "model.layers.66.self_attn.q_proj",
            "model.layers.66.self_attn.v_proj",
            "model.layers.67.self_attn.k_proj",
            "model.layers.67.self_attn.o_proj",
            "model.layers.67.self_attn.q_proj",
            "model.layers.67.self_attn.v_proj",
            "model.layers.68.self_attn.k_proj",
            "model.layers.68.self_attn.o_proj",
            "model.layers.68.self_attn.q_proj",
            "model.layers.68.self_attn.v_proj",
            "model.layers.69.self_attn.k_proj",
            "model.layers.69.self_attn.o_proj",
            "model.layers.69.self_attn.q_proj",
            "model.layers.69.self_attn.v_proj",
            "model.layers.70.self_attn.k_proj",
            "model.layers.70.self_attn.o_proj",
            "model.layers.70.self_attn.q_proj",
            "model.layers.70.self_attn.v_proj",
            "model.layers.71.self_attn.k_proj",
            "model.layers.71.self_attn.o_proj",
            "model.layers.71.self_attn.q_proj",
            "model.layers.71.self_attn.v_proj",
            "model.layers.72.self_attn.k_proj",
            "model.layers.72.self_attn.o_proj",
            "model.layers.72.self_attn.q_proj",
            "model.layers.72.self_attn.v_proj",
            "model.layers.73.self_attn.k_proj",
            "model.layers.73.self_attn.o_proj",
            "model.layers.73.self_attn.q_proj",
            "model.layers.73.self_attn.v_proj",
            "model.layers.74.self_attn.k_proj",
            "model.layers.74.self_attn.o_proj",
            "model.layers.74.self_attn.q_proj",
            "model.layers.74.self_attn.v_proj",
            "model.layers.75.self_attn.k_proj",
            "model.layers.75.self_attn.o_proj",
            "model.layers.75.self_attn.q_proj",
            "model.layers.75.self_attn.v_proj",
            "model.layers.76.self_attn.k_proj",
            "model.layers.76.self_attn.o_proj",
            "model.layers.76.self_attn.q_proj",
            "model.layers.76.self_attn.v_proj",
            "model.layers.77.self_attn.k_proj",
            "model.layers.77.self_attn.o_proj",
            "model.layers.77.self_attn.q_proj",
            "model.layers.77.self_attn.v_proj",
            "model.layers.78.self_attn.k_proj",
            "model.layers.78.self_attn.o_proj",
            "model.layers.78.self_attn.q_proj",
            "model.layers.78.self_attn.v_proj",
            "model.layers.79.self_attn.k_proj",
            "model.layers.79.self_attn.o_proj",
            "model.layers.79.self_attn.q_proj",
            "model.layers.79.self_attn.v_proj",
            "model.layers.80.self_attn.k_proj",
            "model.layers.80.self_attn.o_proj",
            "model.layers.80.self_attn.q_proj",
            "model.layers.80.self_attn.v_proj",
            "model.layers.81.self_attn.k_proj",
            "model.layers.81.self_attn.o_proj",
            "model.layers.81.self_attn.q_proj",
            "model.layers.81.self_attn.v_proj",
            "model.layers.82.self_attn.k_proj",
            "model.layers.82.self_attn.o_proj",
            "model.layers.82.self_attn.q_proj",
            "model.layers.82.self_attn.v_proj",
            "model.layers.83.self_attn.k_proj",
            "model.layers.83.self_attn.o_proj",
            "model.layers.83.self_attn.q_proj",
            "model.layers.83.self_attn.v_proj",
            "model.layers.84.self_attn.k_proj",
            "model.layers.84.self_attn.o_proj",
            "model.layers.84.self_attn.q_proj",
            "model.layers.84.self_attn.v_proj",
            "model.layers.85.self_attn.k_proj",
            "model.layers.85.self_attn.o_proj",
            "model.layers.85.self_attn.q_proj",
            "model.layers.85.self_attn.v_proj",
            "model.layers.86.self_attn.k_proj",
            "model.layers.86.self_attn.o_proj",
            "model.layers.86.self_attn.q_proj",
            "model.layers.86.self_attn.v_proj",
            "model.layers.87.self_attn.k_proj",
            "model.layers.87.self_attn.o_proj",
            "model.layers.87.self_attn.q_proj",
            "model.layers.87.self_attn.v_proj",
            "model.layers.88.self_attn.k_proj",
            "model.layers.88.self_attn.o_proj",
            "model.layers.88.self_attn.q_proj",
            "model.layers.88.self_attn.v_proj",
            "model.layers.89.self_attn.k_proj",
            "model.layers.89.self_attn.o_proj",
            "model.layers.89.self_attn.q_proj",
            "model.layers.89.self_attn.v_proj",
            "model.layers.90.self_attn.k_proj",
            "model.layers.90.self_attn.o_proj",
            "model.layers.90.self_attn.q_proj",
            "model.layers.90.self_attn.v_proj",
            "model.layers.91.self_attn.k_proj",
            "model.layers.91.self_attn.o_proj",
            "model.layers.91.self_attn.q_proj",
            "model.layers.91.self_attn.v_proj",
            "model.layers.92.self_attn.k_proj",
            "model.layers.92.self_attn.o_proj",
            "model.layers.92.self_attn.q_proj",
            "model.layers.92.self_attn.v_proj",
            "model.layers.93.self_attn.k_proj",
            "model.layers.93.self_attn.o_proj",
            "model.layers.93.self_attn.q_proj",
            "model.layers.93.self_attn.v_proj",
            "model.layers.94.self_attn.k_proj",
            "model.layers.94.self_attn.o_proj",
            "model.layers.94.self_attn.q_proj",
            "model.layers.94.self_attn.v_proj",
            "model.layers.95.self_attn.k_proj",
            "model.layers.95.self_attn.o_proj",
            "model.layers.95.self_attn.q_proj",
            "model.layers.95.self_attn.v_proj",
            "model.layers.96.self_attn.k_proj",
            "model.layers.96.self_attn.o_proj",
            "model.layers.96.self_attn.q_proj",
            "model.layers.96.self_attn.v_proj",
            "model.layers.97.self_attn.k_proj",
            "model.layers.97.self_attn.o_proj",
            "model.layers.97.self_attn.q_proj",
            "model.layers.97.self_attn.v_proj",
            "model.layers.98.self_attn.k_proj",
            "model.layers.98.self_attn.o_proj",
            "model.layers.98.self_attn.q_proj",
            "model.layers.98.self_attn.v_proj",
            "model.layers.99.self_attn.k_proj",
            "model.layers.99.self_attn.o_proj",
            "model.layers.99.self_attn.q_proj",
            "model.layers.99.self_attn.v_proj",
            "model.layers.100.self_attn.k_proj",
            "model.layers.100.self_attn.o_proj",
            "model.layers.100.self_attn.q_proj",
            "model.layers.100.self_attn.v_proj",
            "model.layers.101.self_attn.k_proj",
            "model.layers.101.self_attn.o_proj",
            "model.layers.101.self_attn.q_proj",
            "model.layers.101.self_attn.v_proj",
            "model.layers.102.self_attn.k_proj",
            "model.layers.102.self_attn.o_proj",
            "model.layers.102.self_attn.q_proj",
            "model.layers.102.self_attn.v_proj",
            "model.layers.103.self_attn.k_proj",
            "model.layers.103.self_attn.o_proj",
            "model.layers.103.self_attn.q_proj",
            "model.layers.103.self_attn.v_proj",
            "model.layers.104.self_attn.k_proj",
            "model.layers.104.self_attn.o_proj",
            "model.layers.104.self_attn.q_proj",
            "model.layers.104.self_attn.v_proj",
            "model.layers.105.self_attn.k_proj",
            "model.layers.105.self_attn.o_proj",
            "model.layers.105.self_attn.q_proj",
            "model.layers.105.self_attn.v_proj",
            "model.layers.106.self_attn.k_proj",
            "model.layers.106.self_attn.o_proj",
            "model.layers.106.self_attn.q_proj",
            "model.layers.106.self_attn.v_proj",
            "model.layers.107.self_attn.k_proj",
            "model.layers.107.self_attn.o_proj",
            "model.layers.107.self_attn.q_proj",
            "model.layers.107.self_attn.v_proj",
            "model.layers.108.self_attn.k_proj",
            "model.layers.108.self_attn.o_proj",
            "model.layers.108.self_attn.q_proj",
            "model.layers.108.self_attn.v_proj",
            "model.layers.109.self_attn.k_proj",
            "model.layers.109.self_attn.o_proj",
            "model.layers.109.self_attn.q_proj",
            "model.layers.109.self_attn.v_proj",
            "model.layers.110.self_attn.k_proj",
            "model.layers.110.self_attn.o_proj",
            "model.layers.110.self_attn.q_proj",
            "model.layers.110.self_attn.v_proj",
            "model.layers.111.self_attn.k_proj",
            "model.layers.111.self_attn.o_proj",
            "model.layers.111.self_attn.q_proj",
            "model.layers.111.self_attn.v_proj",
            "model.layers.112.self_attn.k_proj",
            "model.layers.112.self_attn.o_proj",
            "model.layers.112.self_attn.q_proj",
            "model.layers.112.self_attn.v_proj",
            "model.layers.113.self_attn.k_proj",
            "model.layers.113.self_attn.o_proj",
            "model.layers.113.self_attn.q_proj",
            "model.layers.113.self_attn.v_proj",
            "model.layers.114.self_attn.k_proj",
            "model.layers.114.self_attn.o_proj",
            "model.layers.114.self_attn.q_proj",
            "model.layers.114.self_attn.v_proj",
            "model.layers.115.self_attn.k_proj",
            "model.layers.115.self_attn.o_proj",
            "model.layers.115.self_attn.q_proj",
            "model.layers.115.self_attn.v_proj",
            "model.layers.116.self_attn.k_proj",
            "model.layers.116.self_attn.o_proj",
            "model.layers.116.self_attn.q_proj",
            "model.layers.116.self_attn.v_proj",
            "model.layers.117.self_attn.k_proj",
            "model.layers.117.self_attn.o_proj",
            "model.layers.117.self_attn.q_proj",
            "model.layers.117.self_attn.v_proj",
            "model.layers.118.self_attn.k_proj",
            "model.layers.118.self_attn.o_proj",
            "model.layers.118.self_attn.q_proj",
            "model.layers.118.self_attn.v_proj",
            "model.layers.119.self_attn.k_proj",
            "model.layers.119.self_attn.o_proj",
            "model.layers.119.self_attn.q_proj",
            "model.layers.119.self_attn.v_proj",
            "model.layers.120.self_attn.k_proj",
            "model.layers.120.self_attn.o_proj",
            "model.layers.120.self_attn.q_proj",
            "model.layers.120.self_attn.v_proj",
            "model.layers.121.self_attn.k_proj",
            "model.layers.121.self_attn.o_proj",
            "model.layers.121.self_attn.q_proj",
            "model.layers.121.self_attn.v_proj",
            "model.layers.122.self_attn.k_proj",
            "model.layers.122.self_attn.o_proj",
            "model.layers.122.self_attn.q_proj",
            "model.layers.122.self_attn.v_proj",
            "model.layers.123.self_attn.k_proj",
            "model.layers.123.self_attn.o_proj",
            "model.layers.123.self_attn.q_proj",
            "model.layers.123.self_attn.v_proj",
            "model.layers.124.self_attn.k_proj",
            "model.layers.124.self_attn.o_proj",
            "model.layers.124.self_attn.q_proj",
            "model.layers.124.self_attn.v_proj",
            "model.layers.125.self_attn.k_proj",
            "model.layers.125.self_attn.o_proj",
            "model.layers.125.self_attn.q_proj",
            "model.layers.125.self_attn.v_proj"
          ],
          "quant_method": "fbgemm_fp8"
        },
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-405B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 16384,
        "initializer_range": 0.02,
        "intermediate_size": 53248,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 128,
        "num_hidden_layers": 126,
        "num_key_value_heads": 16,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-70B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-70B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-8B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": [
          128001,
          128008,
          128009
        ],
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.42.3",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3.1-8B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 131072,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "low_freq_factor": 1.0,
          "high_freq_factor": 4.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.43.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3-70B":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "meta-llama/Meta-Llama-3-70B-Instruct":{
        "architectures": [
          "LlamaForCausalLM"
        ],
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128009,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 8192,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.40.0.dev0",
        "use_cache": true,
        "vocab_size": 128256
      },
      "llava-hf/llava-v1.6-mistral-7b-hf":{
        "architectures": [
          "LlavaNextForConditionalGeneration"
        ],
        "ignore_index": -100,
        "image_grid_pinpoints": [
          [
            336,
            672
          ],
          [
            672,
            336
          ],
          [
            672,
            672
          ],
          [
            1008,
            336
          ],
          [
            336,
            1008
          ]
        ],
        "image_token_index": 32000,
        "model_type": "llava_next",
        "projector_hidden_act": "gelu",
        "text_config": {
          "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
          "architectures": [
            "MistralForCausalLM"
          ],
          "intermediate_size": 14336,
          "max_position_embeddings": 32768,
          "model_type": "mistral",
          "num_key_value_heads": 8,
          "rms_norm_eps": 1e-05,
          "rope_theta": 1000000.0,
          "sliding_window": null,
          "torch_dtype": "bfloat16",
          "vocab_size": 32064
        },
        "torch_dtype": "float16",
        "transformers_version": "4.39.0.dev0",
        "use_image_newline_parameter": true,
        "vision_config": {
          "hidden_size": 1024,
          "image_size": 336,
          "intermediate_size": 4096,
          "model_type": "clip_vision_model",
          "num_attention_heads": 16,
          "num_hidden_layers": 24,
          "patch_size": 14,
          "projection_dim": 768,
          "vocab_size": 32000
        },
        "vision_feature_layer": -2,
        "vision_feature_select_strategy": "default",
        "vocab_size": 32064
      },
      "llava-hf/llama3-llava-next-8b-hf":{
        "architectures": [
          "LlavaNextForConditionalGeneration"
        ],
        "ignore_index": -100,
        "image_grid_pinpoints": [
          [
            336,
            672
          ],
          [
            672,
            336
          ],
          [
            672,
            672
          ],
          [
            1008,
            336
          ],
          [
            336,
            1008
          ]
        ],
        "image_token_index": 128256,
        "model_type": "llava_next",
        "projector_hidden_act": "gelu",
        "text_config": {
          "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
          "architectures": [
            "LlamaForCausalLM"
          ],
          "bos_token_id": 128000,
          "eos_token_id": 128009,
          "intermediate_size": 14336,
          "max_position_embeddings": 8192,
          "model_type": "llama",
          "num_key_value_heads": 8,
          "rms_norm_eps": 1e-05,
          "rope_theta": 500000.0,
          "torch_dtype": "bfloat16",
          "vocab_size": 128320
        },
        "tie_word_embeddings": false,
        "torch_dtype": "float16",
        "transformers_version": "4.42.0.dev0",
        "use_image_newline_parameter": true,
        "vision_config": {
          "hidden_size": 1024,
          "image_size": 336,
          "intermediate_size": 4096,
          "model_type": "clip_vision_model",
          "num_attention_heads": 16,
          "num_hidden_layers": 24,
          "patch_size": 14,
          "projection_dim": 768,
          "vocab_size": 32000
        },
        "vision_feature_layer": -2,
        "vision_feature_select_strategy": "default"
      },
      "mistralai/Mixtral-8x7B-Instruct-v0.1":{
        "architectures": [
          "MixtralForCausalLM"
        ],
        "attention_dropout": 0.0,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 32768,
        "model_type": "mixtral",
        "num_attention_heads": 32,
        "num_experts_per_tok": 2,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "num_local_experts": 8,
        "output_router_logits": false,
        "rms_norm_eps": 1e-05,
        "rope_theta": 1000000.0,
        "router_aux_loss_coef": 0.02,
        "sliding_window": null,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.36.0.dev0",
        "use_cache": true,
        "vocab_size": 32000
      },
      "mistralai/Mixtral-8x7B-v0.1":{
        "architectures": [
          "MixtralForCausalLM"
        ],
        "attention_dropout": 0.0,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 32768,
        "model_type": "mixtral",
        "num_attention_heads": 32,
        "num_experts_per_tok": 2,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "num_local_experts": 8,
        "output_router_logits": false,
        "rms_norm_eps": 1e-05,
        "rope_theta": 1000000.0,
        "router_aux_loss_coef": 0.02,
        "sliding_window": null,
        "tie_word_embeddings": false,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.36.0.dev0",
        "use_cache": true,
        "vocab_size": 32000
      },
      "microsoft/Phi-3.5-vision-instruct": {
          "_name_or_path": "Phi-3.5-vision-instruct",
          "architectures": [
            "Phi3VForCausalLM"
          ],
          "attention_dropout": 0.0,
          "auto_map": {
            "AutoConfig": "configuration_phi3_v.Phi3VConfig",
            "AutoModelForCausalLM": "modeling_phi3_v.Phi3VForCausalLM"
          },
          "bos_token_id": 1,
          "embd_layer": {
            "embedding_cls": "image",
            "hd_transform_order": "sub_glb",
            "projection_cls": "mlp",
            "use_hd_transform": true,
            "with_learnable_separator": true
          },
          "embd_pdrop": 0.0,
          "eos_token_id": 2,
          "hidden_act": "silu",
          "hidden_size": 3072,
          "img_processor": {
            "image_dim_out": 1024,
            "model_name": "openai/clip-vit-large-patch14-336",
            "name": "clip_vision_model",
            "num_img_tokens": 144
          },
          "initializer_range": 0.02,
          "intermediate_size": 8192,
          "max_position_embeddings": 131072,
          "model_type": "phi3_v",
          "num_attention_heads": 32,
          "num_hidden_layers": 32,
          "num_key_value_heads": 32,
          "original_max_position_embeddings": 4096,
          "pad_token_id": 32000,
          "resid_pdrop": 0.0,
          "rms_norm_eps": 1e-05,
          "rope_scaling": {
            "long_factor": [
              1.0800000429153442,
              1.1100000143051147,
              1.1399999856948853,
              1.340000033378601,
              1.5899999141693115,
              1.600000023841858,
              1.6200000047683716,
              2.620000123977661,
              3.2300000190734863,
              3.2300000190734863,
              4.789999961853027,
              7.400000095367432,
              7.700000286102295,
              9.09000015258789,
              12.199999809265137,
              17.670000076293945,
              24.46000099182129,
              28.57000160217285,
              30.420001983642578,
              30.840002059936523,
              32.590003967285156,
              32.93000411987305,
              42.320003509521484,
              44.96000289916992,
              50.340003967285156,
              50.45000457763672,
              57.55000305175781,
              57.93000411987305,
              58.21000289916992,
              60.1400032043457,
              62.61000442504883,
              62.62000274658203,
              62.71000289916992,
              63.1400032043457,
              63.1400032043457,
              63.77000427246094,
              63.93000411987305,
              63.96000289916992,
              63.970001220703125,
              64.02999877929688,
              64.06999969482422,
              64.08000183105469,
              64.12000274658203,
              64.41000366210938,
              64.4800033569336,
              64.51000213623047,
              64.52999877929688,
              64.83999633789062
            ],
            "short_factor": [
               1.08,
              1.1,
              1.1300000000000001,
              1.2800000000000002,
              1.3100000000000003,
              1.4500000000000004,
              1.4500000000000004,
              1.9500000000000008,
              2.030000000000001,
              2.4299999999999926,
              2.5699999999999896,
              2.9499999999999815,
              3.729999999999965,
              3.869999999999962,
              4.189999999999955,
              4.43999999999995,
              4.6399999999999455,
              4.979999999999938,
              5.159999999999934,
              5.279999999999932,
              5.759999999999922,
              5.889999999999919,
              5.889999999999919,
              5.969999999999917,
              6.089999999999915,
              6.2799999999999105,
              6.7699999999999,
              6.8899999999998975,
              7.109999999999893,
              7.129999999999892,
              7.179999999999891,
              7.289999999999889,
              7.339999999999888,
              7.559999999999883,
              7.619999999999882,
              7.69999999999988,
              7.879999999999876,
              7.879999999999876,
              7.879999999999876,
              7.939999999999875,
              7.949999999999875,
              7.979999999999874,
              8.19999999999987,
              8.439999999999864,
              8.469999999999864,
              8.589999999999861,
              8.809999999999857,
              8.999999999999853
            ],
            "type": "su"
          },
          "rope_theta": 10000.0,
          "sliding_window": 262144,
          "tie_word_embeddings": false,
          "torch_dtype": "bfloat16",
          "transformers_version": "4.38.1",
          "use_cache": true,
          "vocab_size": 32064,
          "_attn_implementation": "flash_attention_2"
        }
}
